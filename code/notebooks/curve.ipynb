{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ikea/.local/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/home/ikea/miniconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/ikea/miniconda2/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import eden\n",
    "import matplotlib.pyplot as plt\n",
    "from eden.util import configure_logging\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import tee, chain, islice\n",
    "import numpy as np\n",
    "import random\n",
    "from time import time\n",
    "import datetime\n",
    "from graphlearn.graphlearn import Sampler as GraphLearnSampler\n",
    "from eden.util import fit,estimate\n",
    "from eden.path import Vectorizer\n",
    "import random\n",
    "# get data\n",
    "from eden.converter.graph.gspan import gspan_to_eden\n",
    "from itertools import islice\n",
    "import random\n",
    "'''\n",
    "GET RNA DATA\n",
    "'''\n",
    "from eden.converter.fasta import fasta_to_sequence\n",
    "import itertools\n",
    "\n",
    "def rfam_uri(family_id):\n",
    "    return 'http://rfam.xfam.org/family/%s/alignment?acc=%s&format=fastau&download=0'%(family_id,family_id)\n",
    "def rfam_uri(family_id):\n",
    "    return '%s.fa'%(family_id)\n",
    " \n",
    "\n",
    "from eden.converter.fasta import fasta_to_sequence\n",
    "def get_sequences_with_names(filename='RF00005.fa'):\n",
    "    sequences = fasta_to_sequence(\"../toolsdata/\"+filename)\n",
    "    return sequences\n",
    "\n",
    "\n",
    "def get_graphs(fname,size):\n",
    "    graphs=[g for g in get_sequences_with_names(fname)]\n",
    "    random.shuffle(graphs)\n",
    "    return graphs[:size]\n",
    "\n",
    "# formerly:\n",
    "#get_graphs(dataset_fname, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import graphlearn.abstract_graphs.RNA as rna\n",
    "from  graphlearn.feasibility import FeasibilityChecker as Checker\n",
    "from graphlearn.estimator import Wrapper as estimatorwrapper\n",
    "import graphlearn.utils.draw as draw\n",
    "from graphlearn.graphlearn import Sampler as GLS\n",
    "import itertools\n",
    "\n",
    "\n",
    "\n",
    "def fit_sample(graphs, random_state=random.random()):\n",
    "    '''\n",
    "    graphs -> more graphs\n",
    "    '''\n",
    "    graphs = list(graphs)\n",
    "    estimator=estimatorwrapper( nu=.5, cv=2, n_jobs=-1)\n",
    "    sampler=rna.AbstractSampler(radius_list=[0,1],\n",
    "                                thickness_list=[2], \n",
    "                                min_cip_count=1, \n",
    "                                min_interface_count=2, \n",
    "                                preprocessor=rna.PreProcessor(base_thickness_list=[1],ignore_inserts=True), \n",
    "                                postprocessor=rna.PostProcessor(),\n",
    "                                estimator=estimator\n",
    "                                #feasibility_checker=feasibility\n",
    "                               )\n",
    "    sampler.fit(graphs,grammar_n_jobs=4,grammar_batch_size=1)\n",
    "    \n",
    "    \n",
    "    logger.info('graph grammar stats:')\n",
    "    dataset_size, interface_counts, core_counts, cip_counts = sampler.grammar().size()\n",
    "    logger.info('#instances:%d   #interfaces: %d   #cores: %d   #core-interface-pairs: %d' % (dataset_size, interface_counts, core_counts, cip_counts))\n",
    "    \n",
    "    \n",
    "    graphs = [ b for a ,b in graphs  ]\n",
    "    \n",
    "    graphs = sampler.sample(graphs,\n",
    "                            n_samples=3,\n",
    "                            batch_size=1,\n",
    "                            n_steps=50,\n",
    "                            n_jobs=4,\n",
    "                            quick_skip_orig_cip=True,\n",
    "                            probabilistic_core_choice=True,\n",
    "                            burnin=10,\n",
    "                            improving_threshold=0.9,\n",
    "                            improving_linear_start=0.3,\n",
    "                            max_size_diff=20,\n",
    "                            accept_min_similarity=0.65,\n",
    "                            select_cip_max_tries=30,\n",
    "                            keep_duplicates=False,\n",
    "                            include_seed=False,\n",
    "                            backtrack=10,\n",
    "                            monitor=False)\n",
    "    result=[]\n",
    "    for graphlist in graphs:\n",
    "        result+=graphlist\n",
    "    # note that this is a list [('',sequ),..]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_and_evaluate(pos_original, neg_original,\n",
    "                     pos_sampled, neg_sampled,\n",
    "                     pos_test, neg_test,\n",
    "                     random_state=42):\n",
    "    '''\n",
    "    pos + neg orig+sampled testsets -> orig_roc , sampled_roc, augmented_roc\n",
    "    '''\n",
    "    \n",
    "    # create graph sets...orig augmented and sampled\n",
    "    pos_orig,pos_orig_ = tee(pos_original)\n",
    "    neg_orig,neg_orig_ = tee(neg_original)\n",
    "    \n",
    "    pos_sampled, pos_sampled_ = tee(pos_sampled)\n",
    "    neg_sampled, neg_sampled_ = tee(neg_sampled)\n",
    "    \n",
    "    pos_augmented = chain(pos_orig_,pos_sampled_)\n",
    "    neg_augmented = chain(neg_orig_,neg_sampled_)\n",
    "\n",
    "    predictive_performances = []\n",
    "    for desc,pos_train,neg_train in [('original',pos_orig, neg_orig),\n",
    "                                     ('sample',pos_sampled,neg_sampled),\n",
    "                                     ('original+sample',pos_augmented, neg_augmented)]:\n",
    "        pos_train,pos_train_ = tee(pos_train)\n",
    "        neg_train,neg_train_ = tee(neg_train)\n",
    "        pos_size=sum(1 for x in pos_train_)\n",
    "        neg_size=sum(1 for x in neg_train_)\n",
    "\n",
    "        logger.info( \"-\"*80)\n",
    "        logger.info('working on %s'%(desc))\n",
    "        logger.info('training set sizes: #pos: %d #neg: %d'%(pos_size, neg_size))\n",
    "\n",
    "        if pos_size == 0 or neg_size == 0:\n",
    "            logger.info('WARNING: empty dataset')\n",
    "            predictive_performances.append(0)            \n",
    "        else:\n",
    "            start=time()\n",
    "            pos_test,pos_test_ = tee(pos_test)\n",
    "            neg_test,neg_test_ = tee(neg_test)\n",
    "            \n",
    "            local_estimator = fit(pos_train, neg_train, Vectorizer(4), n_jobs=-1, n_iter_search=1)\n",
    "            apr, roc = estimate(pos_test_, neg_test_, local_estimator, Vectorizer(4))\n",
    "            predictive_performances.append(roc)\n",
    "            logger.info( 'elapsed: %.1f sec'%(time()-start))\n",
    "    return predictive_performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate(pos_fname, neg_fname, size=None, percentages=None, n_repetitions=None, train_test_split=None):\n",
    "    # initializing \n",
    "    graphs_pos = get_graphs(pos_fname, size=size)\n",
    "    graphs_neg = get_graphs(neg_fname, size=size)\n",
    "\n",
    "    # train/test split\n",
    "    from eden.util import random_bipartition_iter\n",
    "    pos_train_global,pos_test_global = random_bipartition_iter(graphs_pos,train_test_split,random_state=random.random()*1000)\n",
    "    neg_train_global,neg_test_global = random_bipartition_iter(graphs_neg,train_test_split,random_state=random.random()*1000)\n",
    "\n",
    "\n",
    "    original_repetitions = []\n",
    "    original_sample_repetitions = []\n",
    "    sample_repetitions = []\n",
    "\n",
    "    for percentage in percentages:\n",
    "        originals = []\n",
    "        originals_samples = []\n",
    "        samples = []\n",
    "        for repetition in range(n_repetitions):\n",
    "            random_state = int(313379*percentage+repetition) \n",
    "            random.seed(random_state)\n",
    "            pos_train_global,pos_train_global_ = tee(pos_train_global)\n",
    "            neg_train_global,neg_train_global_ = tee(neg_train_global)\n",
    "            pos_test_global,pos_test_global_ = tee(pos_test_global)\n",
    "            neg_test_global,neg_test_global_ = tee(neg_test_global)\n",
    "\n",
    "            # use shuffled list to create test and sample set\n",
    "            pos,pos_reminder = random_bipartition_iter(pos_train_global_,percentage)\n",
    "            pos,pos_ = tee(pos)\n",
    "            neg,neg_reminder = random_bipartition_iter(neg_train_global_,percentage)\n",
    "            neg,neg_ = tee(neg)\n",
    "\n",
    "            #sample independently from the 2 classes\n",
    "            logger.info('Positive')\n",
    "            sampled_pos = fit_sample(pos_, random_state=random_state)\n",
    "            logger.info('Negative')\n",
    "            sampled_neg = fit_sample(neg_, random_state=random_state)\n",
    "\n",
    "            #evaluate the predictive performance on held out test set\n",
    "            start=time()\n",
    "            logger.info( \"=\"*80)\n",
    "            logger.info( 'repetition: %d/%d'%(repetition+1, n_repetitions))\n",
    "            logger.info( \"training percentage:\"+str(percentage))\n",
    "            perf_orig,\\\n",
    "            perf_samp,\\\n",
    "            perf_orig_samp = fit_and_evaluate(pos,neg,\n",
    "                                              sampled_pos,sampled_neg,\n",
    "                                              pos_test_global_,neg_test_global_)\n",
    "            logger.info( 'Time elapsed for full repetition: %.1f sec'%((time()-start)))\n",
    "            originals.append(perf_orig)\n",
    "            originals_samples.append(perf_orig_samp)\n",
    "            samples.append(perf_samp)\n",
    "\n",
    "        original_repetitions.append(originals)\n",
    "        original_sample_repetitions.append(originals_samples)\n",
    "        sample_repetitions.append(samples)\n",
    "    \n",
    "    return original_repetitions, original_sample_repetitions, sample_repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot(dataset, percentages, original_sample_repetitions, original_repetitions, sample_repetitions):\n",
    "    gc={'color':'g'}\n",
    "    rc={'color':'r'}\n",
    "    bc={'color':'b'}\n",
    "    FONTSIZE=20\n",
    "    ws = 1\n",
    "    os = np.mean(original_sample_repetitions, axis=1)\n",
    "    o = np.mean(original_repetitions, axis=1)\n",
    "    s = np.mean(sample_repetitions, axis=1)\n",
    "    plt.figure(figsize=(18,8))\n",
    "    ax = plt.subplot() \n",
    "    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        label.set_fontname('Arial')\n",
    "        label.set_fontsize(20)\n",
    "    \n",
    "    \n",
    "    plt.grid()\n",
    "    plt.boxplot(original_sample_repetitions, positions=percentages, widths=ws, capprops=gc, medianprops=gc, boxprops=gc, whiskerprops=gc, flierprops=gc)\n",
    "    plt.plot(percentages,os, color='g', marker='o', markeredgewidth=1, markersize=7, markeredgecolor='g', markerfacecolor='w', label='original+sample')\n",
    "\n",
    "    plt.boxplot(original_repetitions, positions=percentages, widths=ws, capprops=rc, medianprops=rc, boxprops=rc, whiskerprops=rc, flierprops=rc)\n",
    "    plt.plot(percentages,o, color='r', marker='o', markeredgewidth=1, markersize=7, markeredgecolor='r', markerfacecolor='w', label='original')\n",
    "\n",
    "    plt.boxplot(sample_repetitions, positions=percentages, widths=ws, capprops=bc, medianprops=bc, boxprops=bc, whiskerprops=bc, flierprops=bc)\n",
    "    plt.plot(percentages,s, color='b', marker='o', markeredgewidth=1, markersize=7, markeredgecolor='b', markerfacecolor='w', label='sample')\n",
    "\n",
    "    plt.xlim(percentages[0]-.05,percentages[-1]+.05)\n",
    "    plt.xlim(2,15)\n",
    "    #plt.ylim(0.99,1.005)\n",
    "    plt.ylim(0.0,1.005)\n",
    "    plt.title(dataset+'\\n',fontsize=20)\n",
    "    plt.legend(loc='lower right',fontsize=18)\n",
    "    plt.ylabel('ROC AUC',fontsize=18)\n",
    "    plt.xlabel('Training set size per family',fontsize=18)\n",
    "    plt.savefig('%s_plot_predictive_performance_of_samples.pdf' % dataset)\n",
    "\n",
    "#load(\"DATAS\")\n",
    "#plot(\"RF00162 vs RF00005 learning curve\", percentages_l, original_sample_repetitions_l, original_repetitions_l, sample_repetitions_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def save_results(result_fname,percentages, original_repetitions,original_sample_repetitions,sample_repetitions):\n",
    "    with open(result_fname,'w') as f:\n",
    "        f.write('dataset sizes list:\\n')\n",
    "        for perc in percentages:\n",
    "            f.write('%s '% perc)\n",
    "        f.write('\\n')\n",
    "        f.write('AUC scores:\\n')\n",
    "        for repetitions in original_repetitions,original_sample_repetitions,sample_repetitions:\n",
    "            f.write('%s\\n' % len(repetitions))\n",
    "            for repetition in repetitions:\n",
    "                for auc in repetition:\n",
    "                    f.write('%s ' % auc)\n",
    "                f.write('\\n')\n",
    "    \n",
    "def load_results(result_fname):\n",
    "    with open(result_fname) as f:\n",
    "        comment = next(f)\n",
    "        line = next(f)\n",
    "        percentages = [float(x) for x in line.split()]\n",
    "        comment = next(f)\n",
    "\n",
    "        original_repetitions = []\n",
    "        size = int(next(f))\n",
    "        for i in range(size):\n",
    "            line = next(f)\n",
    "            repetition = [float(x) for x in line.split()]\n",
    "            original_repetitions.append(repetition)\n",
    "\n",
    "        original_sample_repetitions = []\n",
    "        size = int(next(f))\n",
    "        for i in range(size):\n",
    "            line = next(f)\n",
    "            repetition = [float(x) for x in line.split()]\n",
    "            original_sample_repetitions.append(repetition)\n",
    "\n",
    "\n",
    "        sample_repetitions = []\n",
    "        size = int(next(f))\n",
    "        for i in range(size):\n",
    "            line = next(f)\n",
    "            repetition = [float(x) for x in line.split()]\n",
    "            sample_repetitions.append(repetition)\n",
    "            \n",
    "    return percentages, original_repetitions,original_sample_repetitions,sample_repetitions\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Experimental pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot(\"RF00162 vs RF01725\", percentages_l, original_sample_repetitions_l, original_repetitions_l, sample_repetitions_l)\n",
    "\n",
    "#print '%s_predictive_performance_of_samples.data'%dataset\n",
    "#!cat \"RF00162 vs RF00005 training curve_predictive_performance_of_samples.data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#percentages_l, original_repetitions_l,original_sample_repetitions_l,sample_repetitions_l = load_results('%s_predictive_performance_of_samples.data'%dataset)\n",
    "#plot(dataset, percentages_l, original_sample_repetitions_l, original_repetitions_l, sample_repetitions_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "%%time\n",
    "#special case: bursi\n",
    "\n",
    "dataset='RF00162 vs RF00005 training curve'\n",
    "#logging\n",
    "logger = logging.getLogger()\n",
    "if True:\n",
    "    logger_fname = '%s_predictive_performance_of_samples.log'%dataset\n",
    "else:\n",
    "    logger_fname = None\n",
    "configure_logging(logger,verbosity=1, filename=logger_fname)\n",
    "\n",
    "#main \n",
    "start=time()\n",
    "print( 'Working with dataset: %s' % dataset )\n",
    "\n",
    "logger.info( 'Working with dataset: %s' % dataset )\n",
    "pos_dataset_fname = 'RF00005.fa'\n",
    "neg_dataset_fname = 'mixed.fa'\n",
    "\n",
    "pos_dataset_fname = 'RF00162.fa'\n",
    "neg_dataset_fname = 'RF01725.fa'\n",
    "\n",
    "percentages=[.08,.2,.4,.6,.8,.95]\n",
    "percentages=[.07,0.1,0.15,0.2]\n",
    "percentages=[.07,.1]\n",
    "\n",
    "\n",
    "# set size to 900 in production\n",
    "original_repetitions,\\\n",
    "original_sample_repetitions,\\\n",
    "sample_repetitions = evaluate(pos_dataset_fname,\n",
    "                              neg_dataset_fname,\n",
    "                              size=100,\n",
    "                              percentages=percentages,\n",
    "                              n_repetitions=6, # ORIG = 10\n",
    "                              train_test_split=0.7)\n",
    "#save and display results\n",
    "result_fname='%s_predictive_performance_of_samples.data'%dataset\n",
    "save_results(result_fname,percentages, original_repetitions,original_sample_repetitions,sample_repetitions) \n",
    "\n",
    "\n",
    "percentages_l, original_repetitions_l,original_sample_repetitions_l,sample_repetitions_l = load_results('%s_predictive_performance_of_samples.data'%dataset)\n",
    "plot(dataset, percentages_l, original_sample_repetitions_l, original_repetitions_l, sample_repetitions_l)\n",
    "\n",
    "print('Time elapsed: %s'%(datetime.timedelta(seconds=(time() - start))))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''time\n",
    "for dataset in dataset_names:\n",
    "    #logging\n",
    "    logger = logging.getLogger()\n",
    "    if True:\n",
    "        logger_fname = '%s_predictive_performance_of_samples.log'%dataset\n",
    "    else:\n",
    "        logger_fname = None\n",
    "    configure_logging(logger,verbosity=1, filename=logger_fname)\n",
    "    \n",
    "    #main \n",
    "    start=time()\n",
    "    print( 'Working with dataset: %s' % dataset )\n",
    "\n",
    "    logger.info( 'Working with dataset: %s' % dataset )\n",
    "    pos_dataset_fname = \"RF00005.fa\"\n",
    "    neg_dataset_fname = 'mixed.fa\n",
    "\n",
    "    percentages=[.05,.2,.4,.6,.8,.95]\n",
    "    percentages=[.05,.2]\n",
    "\n",
    "    original_repetitions,\\\n",
    "    original_sample_repetitions,\\\n",
    "    sample_repetitions = evaluate(pos_dataset_fname,\n",
    "                                  neg_dataset_fname,\n",
    "                                  size=400,\n",
    "                                  percentages=percentages,\n",
    "                                  n_repetitions=3,\n",
    "                                  train_test_split=0.7)\n",
    "    #save and display results\n",
    "    result_fname='%s_predictive_performance_of_samples.data'%dataset\n",
    "    save_results(result_fname,percentages, original_repetitions,original_sample_repetitions,sample_repetitions)    \n",
    "    percentages_l, original_repetitions_l,original_sample_repetitions_l,sample_repetitions_l = load_results(result_fname)\n",
    "    plot(dataset, percentages_l, original_sample_repetitions_l, original_repetitions_l, sample_repetitions_l)\n",
    "    \n",
    "    print('Time elapsed: %s'%(datetime.timedelta(seconds=(time() - start))))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#display\n",
    "'''\n",
    "for dataset in dataset_names:\n",
    "    result_fname='%s_predictive_performance_of_samples.data'%dataset\n",
    "    percentages_l, original_repetitions_l,original_sample_repetitions_l,sample_repetitions_l = load_results(result_fname)\n",
    "    plot(dataset, percentages_l, original_sample_repetitions_l, original_repetitions_l, sample_repetitions_l)\n",
    "''' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from eden.converter.fasta import fasta_to_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "# we always want a test and a train set to omou\n",
    "def get_seq_tups(fname,size,sizeb):\n",
    "    kram = fasta_to_sequence(\"../toolsdata/\"+fname)\n",
    "    graphs=[g for g in kram]\n",
    "    random.shuffle(graphs)\n",
    "    return graphs[:size],graphs[size:size+sizeb]\n",
    "\n",
    "def plot(dataset, percentages, original_sample_repetitions, original_repetitions, sample_repetitions): # note that the var names are not real anymore.\n",
    "    gc={'color':'g'}\n",
    "    rc={'color':'r'}\n",
    "    bc={'color':'b'}\n",
    "    FONTSIZE=20\n",
    "    ws = .3\n",
    "    os = np.mean(original_sample_repetitions, axis=1)\n",
    "    o = np.mean(original_repetitions, axis=1)\n",
    "    s = np.mean(sample_repetitions, axis=1)\n",
    "    plt.figure(figsize=(18,8))\n",
    "    ax = plt.subplot() \n",
    "    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        label.set_fontname('Arial')\n",
    "        label.set_fontsize(20)\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.boxplot(original_sample_repetitions, positions=percentages, widths=ws, capprops=gc, medianprops=gc, boxprops=gc, whiskerprops=gc, flierprops=gc)\n",
    "    plt.plot(percentages,os, color='g', marker='o', markeredgewidth=1, markersize=7, markeredgecolor='g', markerfacecolor='w', label='original')\n",
    "\n",
    "    plt.boxplot(original_repetitions, positions=percentages, widths=ws, capprops=rc, medianprops=rc, boxprops=rc, whiskerprops=rc, flierprops=rc)\n",
    "    plt.plot(percentages,o, color='r', marker='o', markeredgewidth=1, markersize=7, markeredgecolor='r', markerfacecolor='w', label='sample')\n",
    "\n",
    "    plt.boxplot(sample_repetitions, positions=percentages, widths=ws, capprops=bc, medianprops=bc, boxprops=bc, whiskerprops=bc, flierprops=bc)\n",
    "    plt.plot(percentages,s, color='b', marker='o', markeredgewidth=1, markersize=7, markeredgecolor='b', markerfacecolor='w', label='sample+orig')\n",
    "\n",
    "    # testing to plot some dots\n",
    "    global similarity_scores\n",
    "    plt.plot(percentages,similarity_scores,'bo')\n",
    "    \n",
    "    plt.xlim(percentages[0]-.05,percentages[-1]+.05)\n",
    "    plt.xlim(5,20)\n",
    "    plt.ylim(0.0,1.005)\n",
    "    plt.title(dataset+'\\n',fontsize=20)\n",
    "    plt.legend(loc='lower right',fontsize=18)\n",
    "    plt.ylabel('ROC AUC',fontsize=18)\n",
    "    plt.xlabel('Training set size per family',fontsize=18)\n",
    "    plt.savefig('%s_plot_predictive_performance_of_samples.pdf' % dataset)\n",
    "\n",
    "#load(\"DATAS\")\n",
    "#plot(\"RF00162 vs RF00005 learning curve\", [30,70], [[.30,.30],[.20,.20]] , [[.40,.40],[.30,.30]],[[.70,.35],[.25,.25]])\n",
    "import random\n",
    "import graphlearn.abstract_graphs.RNA as rna\n",
    "from  graphlearn.feasibility import FeasibilityChecker as Checker\n",
    "from graphlearn.estimator import Wrapper as estimatorwrapper\n",
    "import graphlearn.utils.draw as draw\n",
    "from graphlearn.graphlearn import Sampler as GLS\n",
    "import itertools\n",
    "\n",
    "\n",
    "\n",
    "def fit_sample(graphs, random_state=random.random()):\n",
    "    '''\n",
    "    graphs -> more graphs\n",
    "    '''\n",
    "    graphs = list(graphs)\n",
    "    estimator=estimatorwrapper( nu=.5, cv=2, n_jobs=-1)\n",
    "    sampler=rna.AbstractSampler(radius_list=[0,1],\n",
    "                                thickness_list=[2], \n",
    "                                min_cip_count=1, \n",
    "                                min_interface_count=2, \n",
    "                                preprocessor=rna.PreProcessor(base_thickness_list=[1],ignore_inserts=True), \n",
    "                                postprocessor=rna.PostProcessor(),\n",
    "                                estimator=estimator\n",
    "                                #feasibility_checker=feasibility\n",
    "                               )\n",
    "    sampler.fit(graphs,grammar_n_jobs=4,grammar_batch_size=1)\n",
    "    graphs = [ b for a ,b in graphs  ]\n",
    "    graphs = sampler.sample(graphs,\n",
    "                            n_samples=3,\n",
    "                            batch_size=1,\n",
    "                            n_steps=50,\n",
    "                            n_jobs=4,\n",
    "                            quick_skip_orig_cip=True,\n",
    "                            probabilistic_core_choice=True,\n",
    "                            burnin=10,\n",
    "                            improving_threshold=0.3,\n",
    "                            improving_linear_start=0.2,\n",
    "                            max_size_diff=20,\n",
    "                            accept_min_similarity=0.65,\n",
    "                            select_cip_max_tries=30,\n",
    "                            keep_duplicates=False,\n",
    "                            include_seed=False,\n",
    "                            backtrack=2,\n",
    "                            monitor=False)\n",
    "    result=[]\n",
    "    for graphlist in graphs:\n",
    "        result+=graphlist\n",
    "    # note that this is a list [('',sequ),..]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "#  ok erstmal ueber alle x values, ne\n",
    "size_test=20\n",
    "dataset_a='RF00005.fa'\n",
    "dataset_a='RF01725.fa'\n",
    "dataset_b='RF00162.fa'\n",
    "sizes=[7,8,9,10,11,12,13,14,15]\n",
    "sizes=[7,8]\n",
    "repeats=1\n",
    "\n",
    "# calc everything\n",
    "def get_results():\n",
    "    li = [ get_datapoint(size) for size in sizes ]\n",
    "    # transpose , should work OO \n",
    "    print 'li:',li\n",
    "    return [list(i) for i in zip(*li)]\n",
    "\n",
    "from graphlearn import sumsim\n",
    "# calc for one \"size\", go over repeats\n",
    "def get_datapoint(size):\n",
    "    ra=[]\n",
    "    rb=[]\n",
    "    rab=[]\n",
    "    similarities=[]\n",
    "    global similarity_scores\n",
    "    \n",
    "    for rep in range(repeats):\n",
    "        train_a,test_a = get_seq_tups(dataset_a,size,size_test)\n",
    "        train_b,test_b = get_seq_tups(dataset_b,size,size_test)\n",
    "        a,b,ab,similarity = evaluate_point(train_a,train_b,test_a,test_b)\n",
    "        ra.append(a)\n",
    "        rab.append(ab)\n",
    "        rb.append(b)\n",
    "        similarities.append(similarity)\n",
    "\n",
    "    similarity_scores.append( (sum(similarities)/float(len(similarities))))\n",
    "    return ra,rb,rab\n",
    "\n",
    "\n",
    "def evaluate_point(train_a,train_b,test_a,test_b):\n",
    "    res=[]\n",
    "    res.append(  test(deepcopy(train_a),deepcopy(train_b),deepcopy(test_a),deepcopy(test_b)) )\n",
    "    train_aa = fit_sample(train_a)\n",
    "    train_bb = fit_sample(train_b)\n",
    "    \n",
    "    eins=sumsim.calcsimset(deepcopy(train_aa),deepcopy(train_a))   \n",
    "    zwei=sumsim.calcsimset(deepcopy(train_bb),deepcopy(train_b))   \n",
    "    drei = (eins+zwei)/2.0\n",
    "    res.append(  test(deepcopy(train_aa),deepcopy(train_bb),deepcopy(test_a),deepcopy(test_b)) )\n",
    "    res.append(  test(deepcopy(train_a)+deepcopy(train_aa),deepcopy(train_b)+train_bb,deepcopy(test_a),deepcopy(test_b)) )\n",
    "    res.append(drei)\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "### just evaluate the stuff\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from eden.path import Vectorizer\n",
    "\n",
    "def train_esti(neg,pos):\n",
    "        v=Vectorizer()\n",
    "        matrix=v.transform(neg+pos)\n",
    "        res=SGDClassifier(shuffle=True)\n",
    "        res.fit(matrix, np.asarray(  [-1]*len(neg)+[1]*len(pos)  ) )\n",
    "        return res\n",
    "\n",
    "def eva(esti,ne,po):\n",
    "    v=Vectorizer()\n",
    "    matrix=v.transform(ne)\n",
    "    correct= sum(  [1 for res in esti.predict(matrix) if res == -1] ) \n",
    "    matrix2=v.transform(po)            \n",
    "    correct+= sum(  [1 for res in esti.predict(matrix2) if res == 1] )\n",
    "    return correct\n",
    "\n",
    "def test(a,b,ta,tb):\n",
    "    est=train_esti(a,b)\n",
    "    correct=eva(est,ta,tb)\n",
    "    return correct/float(size_test*2) # fraction correct\n",
    "    \n",
    "\n",
    "global similarity_scores\n",
    "similarity_scores=[]\n",
    "r=get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot('somename', sizes, *r)\n",
    "\n",
    "print similarity_scores\n",
    "print sizes\n",
    "print r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def levenshtein(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein(s2, s1)\n",
    "    # len(s1) >= len(s2)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1 # j+1 instead of j since previous_row and current_row are one character longer\n",
    "            deletions = current_row[j] + 1       # than s2\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    return previous_row[-1]\n",
    "\n",
    "\n",
    "def sim(s1,s2):\n",
    "    l = float( max(len(s1),len(s2))) \n",
    "    lp = l - float(levenshtein(s1,s2))\n",
    "    return lp/l \n",
    "\n",
    "\n",
    "print 'testing sim eq', sim(s1[0],s1[0])\n",
    "print 'testing sim dif', sim(s2[0],s1[0])\n",
    "\n",
    "s1=['asdasd','asdasd','abc']\n",
    "s2=['zxczxc','asdasd','abc']\n",
    "\n",
    "\n",
    "def simsum(a,b,del_diag=False):\n",
    "    res=0.0\n",
    "    for i, ea in enumerate(a):\n",
    "        for j, eb in enumerate(b):\n",
    "            if del_diag and i==j:\n",
    "                continue\n",
    "            res+=simmilarity(ea,eb)\n",
    "    return res\n",
    "\n",
    "\n",
    "print 'testing simsum eq',simsum(s1,s1,True)\n",
    "print 'testing simsum neq',simsum(s2,s1)\n",
    "\n",
    "import math\n",
    "def calcsimset(a,b):\n",
    "    print 'calcsimset'\n",
    "    ab=simsum(s1,s2,False)\n",
    "    aa=simsum(a,a,False) \n",
    "    bb=simsum(b,b,False)\n",
    "    print ab,aa,bb\n",
    "    cc=aa*bb\n",
    "    print cc\n",
    "    print ab/math.sqrt(cc)\n",
    "\n",
    "    \n",
    "calcsimset(s1,s2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from graphlearn import sumsim\n",
    "print sumsim.__file__\n",
    "s1=['asdasd','asdasd','abc']\n",
    "s2=['zxczxc','asdasd','abc']\n",
    "sumsim.calcsimset(s1,s2)\n",
    "sumsim.similarity_mean(s1,s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "s1 = [('', 'CUCUUAUUGAGAGCGGUGGAGGGACUGGCCCUGUGAAACCCGGCAACCUUCAAACGAAAUGUUUGAAACGGUGCUAAUACCUGCAAAACGAAUGUUUUGCAUAAUAAGAG'), ('', 'CUCUUAUCCUGAGUGGCGGAGGGACAGGCCCAAUGAAGCCCAGCAACCUCUUCUUAAUUGAAGAAAGGUGCCAAACCGUUUGCAGACAAAUAGCGUCUGAACGAUAAGAG'), ('', 'CUCUUAUCCUGAGUGGCGGAGGGAAAGGCCCAAUGAAGCCCAGCAACCUCUUCUUAAUUGAAGAAAGGUGCCAAACCGUUUGCAGACAAAUAGCGUCUGAACGAUAAGAG'), ('', 'CUCUUAUCCUGAGUGGCGGAGGGAAACGGCCCAAUGAAGCCCAGCAACCUCUUCUUAAUUGAAGAAAGGUGCCAAACCGUUUGCAGACAAAUAGCGUCUGAACGAUAAGAG'), ('', 'AGCUCAUCCAGAGGGGCAGAGGGAAACGGCCCGAUGAAGCCCCGGCAACCCUCCAGUCGGUACUCGUAGGUCACUGAUUAUGAUAGGGAAGGUGCCAAAUCCGUCUCACGGCGAGAUGCGUCGUGAGGAAGAUGAGGA'), ('', 'UGCUUAUCUAGAGUGGCGGAGGGAAACGGCCCUUUGAAGCCCAGCAACCUCUUCUUAAUUGAAGAAAGGUGCCAAUUCCAGACAGAUGAGGA'), ('', 'UUCUUAUCAAGAGUUGGCGGAGAUACAAGGAUCUGUGAUGCCACAGCAACCUAUAUUUAUUAUGUGGUGCUAAUUCCUUUUGGCAAAAUCUAAGCCUGAAAGAUGAGAA'), ('', 'CUUUUAUCCAGAGAUGGCGGAGGGACAGGCCCGAAGAAGCCCAGCAACCUCUUCUUAAUUGAAGAAAGGUGCCAACCUGCAGAAUGCUCGGCGUUCUGGAAGAUAAGAG'), ('', 'CUUUUAUCCAGAGAUGGCGGAGGGAAAGGCCCGAAGAAGCCCAGCAACCUCUUCGUAACGAAGAAAGGUGCCAACCUGCAGAAUGCUCGGCGUUCUGGAAGAUAAGAG'), ('', 'AGCUUAUCGAGAGUUGGCGGAGAUACAAGGAUCUGUGAUGCCACAGCAACCUAUAUUUAUUAUGUGGUGCUAAUUCCCAUCCCGAAUAUUCGGGAAUAGAUGAGCG')] \n",
    "s2 = [('ACEZ01000126.1/65345-65190', 'AGCUCAUCCAGAGGGGCAGAGGGAAACGGCCCGAUGAAGCCCCGGCAACCCUCCAGUCGGUACUCGUAGGUCACUGGCGACCACUUCGCGAGGCUCCCGACUAGGGAAGGUGCCAAAUCCGUCUCACGGCGAGAUGCGUCGUGAGGAAGAUGAGGA'), ('AP008934.1/1011112-1011223', 'CUCUUAUCCUGAGUGGCGGAGGGACAUGGACCCAAUGAAGCCCAGCAACCUCUUCUUAAUUGAAGAAAGGUGCCAAACCGUUUGCAGACAAAUAGCGUCUGAACGAUAAGAG'), ('AAVL02000036.1/100921-100817', 'CUCUUAUUAAGAGUUGGCGGAGAUACAAGGAUCUGUGAUGCCACGGCAACCCCCGAUUAUGAUGGAAGGUGCCCACCGGAGCAAUGCAAUAUUGAUCAAUAAGAG'), ('AE017225.1/4074580-4074471', 'CUCUUAUUGAGAGCGGUGGAGGGAAAGGCCCUGUGAAACCCGGCAACCUUCAAACGAAAUGUUUGAAACGGUGCUAAUACCUGCAAAACGAAUGUUUUGCAUAAUAAGAG'), ('ABDQ01000003.1/211832-211918', 'UGCUUAUCUAGAGUGGCGGAGGGACUGGCCCUUUGAAGCCCAGCAACCUAUAUUUAUUAUGUGGUGCUAAUUCCAGACAGAUGAGGA'), ('AP006627.1/3008449-3008342', 'CUUUUAUCCAGAGAUGGCGGAGGGACAGGCCCGAAGAAGCCCAGCAACCAACACGUAACGUGUAAAGGUGCUAACCUGCAGAAUGCUCGGCGUUCUGGAAGAUAAGAG'), ('AAXV01000005.1/126573-126679', 'UUCUUAUCAAGAGAGACGGAGGGAUCGGCCCGAUGAAGUCUCAGCAACCAGCUCAAUCAGUAUGGUGCUAAUUCCUUUUGGCAAAAUCUAAGCCUGAAAGAUGAGAA'), ('AAXU02000001.1/3185191-3185093', 'AGCUUAUCGAGAAAGACUGAGGGAAGGGCCCGACGACGUCUUAGCAACCUGUAACCAAGGUGCUAAUUCCCAUCCCGAAUAUUCGGGAAUAGAUGAGCG')]\n",
    "\n",
    "\n",
    "from Valium import sumsim as ss\n",
    "print ss.__file__\n",
    "#sumsim.calcsimset(s1,s2)\n",
    "#ss.score(s1,s2)\n",
    "a,b = ss.vectorize(s1,s2)\n",
    "dist= ss.compdistr(a,b)\n",
    "print dist\n",
    "sim = ss.simset(a,b)\n",
    "print 'sim',sim\n",
    "print dist-sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a=np.ones(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_datapoint(size):\n",
    "    ra=[]\n",
    "    rb=[]\n",
    "    rab=[]\n",
    "    similarities=[]\n",
    "    global similarity_scores\n",
    "    for rep in range(repeats):\n",
    "\n",
    "\n",
    "        a,b,ab,similarity = evaluate_point(size)\n",
    "        ra.append(a)\n",
    "        rab.append(ab)\n",
    "        rb.append(b)\n",
    "        similarities.append(similarity)\n",
    "        \n",
    "    similarity_scores.append( \n",
    "        (sum(similarities)/float(len(similarities)))\n",
    "        )\n",
    "    return ra,rb,rab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_point(size):\n",
    "    res=[]\n",
    "\n",
    "    train_aa,train_a,test_a = get_trainthings(size,dataset_a)\n",
    "    train_bb,train_b,test_b = get_trainthings(size,dataset_b)\n",
    "\n",
    "\n",
    "    res.append(  \n",
    "        test(deepcopy(train_a),deepcopy(train_b),deepcopy(test_a),deepcopy(test_b)) \n",
    "    )\n",
    "    eins=sumsim.simset(deepcopy(train_aa),deepcopy(train_a))\n",
    "    zwei=sumsim.simset(deepcopy(train_bb),deepcopy(train_b)) \n",
    "    drei = (eins+zwei)/2.0\n",
    "    res.append(  test(deepcopy(train_aa),deepcopy(train_bb),deepcopy(test_a),deepcopy(test_b)) )\n",
    "    res.append(  test(deepcopy(train_a)+deepcopy(train_aa),deepcopy(train_b)+train_bb,deepcopy(test_a),deepcopy(test_b)) )\n",
    "    res.append(drei)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import curve\n",
    "\n",
    "sizes = [20, 25]\n",
    "result = [[[0.95], [1.0]], [[0.975], [0.9]], [[0.95], [1.0]]]\n",
    "\n",
    "sizes = [20, 25]\n",
    "result = [[[1.0, 0.95, 1.0], [1.0, 0.975, 1.0]], [[0.775, 0.775, 0.925], [1.0, 1.0, 0.925]], [[0.975, 0.95, 0.975], [0.875, 0.975, 1.0]]]\n",
    "similarity_scores = [1.0297192402202902, 1.0271841248414073]\n",
    "curve.similarity_scores = similarity_scores \n",
    "\n",
    "result = [\n",
    "            [\n",
    "                [1.0, 1.0, 0.975, 0.975, 0.95, 1.0, 1.0, 1.0, 1.0], \n",
    "                [1.0, 0.95, 1.0, 1.0, 1.0, 0.975, 0.975, 1.0, 0.975], \n",
    "                [0.975, 1.0, 1.0, 0.975, 0.95, 1.0, 1.0, 1.0, 1.0],\n",
    "                [1.0, 1.0, 1.0, 0.975, 1.0, 1.0, 1.0, 1.0, 1.0], \n",
    "                [0.925, 0.95, 0.975, 1.0, 1.0, 0.975, 1.0, 1.0, 0.975], \n",
    "                [1.0, 0.975, 1.0, 1.0, 1.0, 1.0, 0.975, 1.0, 1.0], \n",
    "                [0.975, 1.0, 1.0, 1.0, 0.975, 1.0, 1.0, 1.0, 1.0]], \n",
    "            [\n",
    "                [0.825, 1.0, 0.9, 1.0, 0.975, 0.825, 0.725, 0.6, 1.0], \n",
    "                [0.675, 0.9, 0.775, 1.0, 0.85, 1.0, 0.8, 0.85, 0.975], \n",
    "                [1.0, 1.0, 0.975, 0.975, 0.9, 0.975, 1.0, 1.0, 1.0], \n",
    "                [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.975, 1.0], \n",
    "                [1.0, 1.0, 1.0, 0.975, 0.95, 1.0, 1.0, 1.0, 1.0], \n",
    "                [0.975, 0.975, 1.0, 1.0, 0.975, 0.975, 0.975, 0.975, 1.0], \n",
    "                [1.0, 1.0, 0.975, 1.0, 1.0, 1.0, 1.0, 1.0, 0.975]], \n",
    "           [\n",
    "                [0.975, 0.975, 1.0, 0.975, 0.975, 1.0, 1.0, 1.0, 1.0], \n",
    "                [1.0, 1.0, 1.0, 1.0, 1.0, 0.975, 0.95, 1.0, 1.0], \n",
    "                [1.0, 1.0, 0.95, 0.975, 0.975, 1.0, 0.95, 1.0, 1.0], \n",
    "                [1.0, 0.975, 1.0, 0.975, 1.0, 0.975, 1.0, 1.0, 1.0], \n",
    "                [0.975, 1.0, 1.0, 0.875, 1.0, 1.0, 1.0, 1.0, 1.0], \n",
    "                [1.0, 0.975, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], \n",
    "                [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]]\n",
    "        ]\n",
    "similarity_scores = [1.0232968855718843, 1.0007455908832672, 0.99180448653574027, 0.98595714838649307, 0.9916254516732721, 0.98422063083413702, 0.98257742970417483]\n",
    "\n",
    "result = [[[1.0, 0.975, 0.975, 1.0, 0.975, 0.975, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 0.95, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 0.975, 1.0, 1.0, 1.0, 1.0], [0.95, 1.0, 0.975, 1.0, 0.975, 1.0, 0.975, 0.975, 1.0], [1.0, 0.925, 1.0, 1.0, 1.0, 1.0, 1.0, 0.975, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.975], [1.0, 1.0, 1.0, 0.975, 1.0, 1.0, 1.0, 1.0, 0.975]], [[0.975, 0.975, 1.0, 0.825, 0.925, 0.925, 0.95, 1.0, 0.975], [1.0, 1.0, 1.0, 0.875, 1.0, 1.0, 0.95, 1.0, 0.975], [1.0, 0.95, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.975], [1.0, 0.975, 0.95, 1.0, 1.0, 1.0, 0.975, 1.0, 1.0], [0.975, 0.95, 0.975, 1.0, 1.0, 0.975, 0.975, 0.975, 0.975], [1.0, 1.0, 1.0, 1.0, 0.975, 0.95, 1.0, 1.0, 0.925], [1.0, 1.0, 1.0, 0.975, 1.0, 0.975, 1.0, 1.0, 0.975]], [[1.0, 1.0, 0.975, 1.0, 1.0, 1.0, 0.85, 1.0, 1.0], [1.0, 1.0, 0.975, 0.975, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 0.975, 1.0, 1.0, 0.975, 1.0, 1.0, 0.95], [1.0, 1.0, 0.825, 1.0, 1.0, 1.0, 0.975, 0.975, 1.0], [0.975, 1.0, 0.975, 0.975, 0.875, 1.0, 0.975, 1.0, 0.975], [1.0, 1.0, 1.0, 0.975, 1.0, 0.95, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]]]\n",
    "similarity_scores = [1.0168101311663511, 1.0150377354846287, 1.0009668568440384, 1.0013156444262343, 0.99809664727147385, 0.99622003788844737, 0.99512742871661353]\n",
    "\n",
    "numgr=[20 ,25 ,30, 35, 40, 45, 50]\n",
    "\n",
    "result = [[[0.99, 0.985, 0.985, 0.99, 0.99, 0.985, 0.985], [0.955, 0.985, 1.0, 0.99, 0.97, 0.975, 0.975], [0.985, 0.975, 0.95, 0.965, 0.965, 0.985, 0.96], [0.975, 0.955, 0.975, 0.995, 0.98, 0.975, 0.995], [0.99, 0.99, 0.99, 0.985, 0.98, 0.99, 0.985], [0.98, 0.965, 0.975, 0.98, 0.97, 0.985, 0.99], [0.995, 0.975, 0.98, 0.99, 0.995, 0.985, 0.995]], [[0.995, 0.985, 0.835, 0.78, 0.91, 0.965, 0.975], [0.995, 0.93, 0.985, 0.99, 0.98, 0.985, 0.98], [0.965, 0.845, 0.98, 0.95, 0.925, 0.975, 0.985], [0.975, 0.985, 0.975, 0.97, 0.955, 0.985, 0.985], [0.995, 0.99, 0.995, 0.985, 0.99, 0.98, 0.99], [0.99, 1.0, 0.985, 0.99, 0.995, 0.98, 0.995], [0.99, 0.985, 0.945, 0.945, 0.985, 0.99, 0.985]], [[0.99, 0.995, 1.0, 0.995, 0.99, 0.995, 1.0], [1.0, 0.98, 0.975, 0.945, 0.99, 1.0, 0.98], [0.985, 0.985, 0.975, 0.995, 0.98, 0.975, 0.97], [0.995, 0.985, 0.985, 0.985, 0.98, 0.975, 0.975], [0.985, 0.995, 0.985, 0.995, 0.995, 0.995, 0.98], [0.985, 0.995, 0.995, 0.99, 0.98, 0.99, 0.995], [0.995, 0.99, 0.995, 0.99, 0.995, 0.98, 0.985]]]\n",
    "similarity_scores = [1.009134975438486, 1.0020476673182956, 0.99512728715033838, 0.9902838600590177, 0.98878387701645598, 0.98004841660025499, 0.97223030477865113]\n",
    "\n",
    "curve.similarity_scores=similarity_scores\n",
    "curve.plot(\"%s vs %s discriminative performance\" % (curve.dataset_a[:-3],curve.dataset_b[:-3]),numgr,*result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l=np.array(range(4))\n",
    "l.tolist()*7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ikea/.local/lib/python2.7/site-packages/matplotlib/__init__.py:1357: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". k [ 0.0443629   0.32738144  0.80030082  0.18683276  0.17653465  0.72132509\n",
      "  0.10689771  0.09973688  0.11484122  0.07831989  0.75110302  0.85493524\n",
      "  0.10236151  0.06671896  0.06108424  0.05318691  0.18290676  0.06248424\n",
      "  0.2307442   0.06805646  0.46870439  0.14099067  0.78874497  0.45324056\n",
      "  0.18515714  0.16364589  0.6900455   0.14484918  0.13691777  0.56736255\n",
      "  0.41750974  0.10341451  0.70801311  0.73601176]\n",
      "k [ 0.05159334  0.05368092  0.08238888  0.07573681  0.2439573   0.77715203\n",
      "  0.18849626  0.1401582   0.74672336  0.0942311   0.2417722   0.07492825\n",
      "  0.80357484  0.0640926   0.82464191  0.16081312  0.09680532  0.0427042\n",
      "  0.7447316   0.0621928   0.76950495  0.08569051  0.298003    0.22048796\n",
      "  0.76610386  0.30822068  0.71626188  0.72844452  0.30930807  0.12317854\n",
      "  0.10662314  0.39622649  0.63096642  0.17448032  0.6681357   0.76647985]\n",
      "k [ 0.20197516  0.24125152  0.39624697  0.38373754  0.38955718  0.24750676\n",
      "  0.64125622  0.6411006   0.16140893  0.26992794  0.23126367  0.16456038\n",
      "  0.14099046  0.14303202  0.14769458  0.23799027  0.26493787  0.20051647\n",
      "  0.64114952  0.20723231  0.64135075  0.16508963  0.4537526   0.52851518\n",
      "  0.60977247  0.62628676  0.64410422  0.6414688   0.53177408  0.16483995\n",
      "  0.16842831  0.40313789  0.61542202  0.22829031  0.14886104  0.6264625 ]\n",
      "k [ 0.48240714  0.25415065  0.18446217  0.23119928  0.19446506  0.09291302\n",
      "  0.12884229  0.1673609   0.14265945  0.11208577  0.09414403  0.50008269\n",
      "  0.15559542  0.15400164  0.8691755   0.07058211  0.05656976  0.05674452\n",
      "  0.23802941  0.23833552  0.15619226  0.27095976  0.29800543  0.76623766\n",
      "  0.47777707  0.20000195  0.17872915  0.8537055   0.89252685  0.92264154\n",
      "  0.82683358  0.20068361  0.19452247  0.77891216  0.26876592  0.39345425\n",
      "  0.41438221  0.06669111  0.88053163  0.85048298  0.57175014  0.57143909\n",
      "  0.08576988  0.09086125  0.0843126   0.12334967  0.1165316   0.11170852\n",
      "  0.29805995  0.70729209  0.17409905  0.18994409  0.06570358  0.06187134\n",
      "  0.91686088  0.19896637  0.17217387  0.58586658  0.54993484  0.91101929\n",
      "  0.06961785  0.06187134  0.91307937  0.07970774  0.09966586  0.09196952\n",
      "  0.0843126   0.06649618  0.07202836  0.0724458   0.28757725  0.30669497\n",
      "  0.12614637  0.2439538   0.10554386  0.14258363  0.90093049  0.38796169\n",
      "  0.92340288  0.85746985  0.10569657  0.10239697  0.73980559  0.41174562\n",
      "  0.30642875  0.8687266   0.87418738  0.20925233  0.71598458  0.89431172\n",
      "  0.90792004  0.50336182  0.83487417  0.44769517  0.52805612  0.07285667\n",
      "  0.34365063  0.91805879  0.11925021  0.11885006  0.57702184  0.92410764\n",
      "  0.4760808   0.77266397  0.34365063  0.06784032  0.22549413  0.93168777\n",
      "  0.20185362  0.57233931  0.1094784   0.54650511  0.88184708  0.56072744\n",
      "  0.12249047  0.100633    0.2335745   0.94099378  0.10108501  0.60030906]\n",
      "k [ 0.05780929  0.05197243  0.16001943  0.03524413  0.04162629  0.10345113\n",
      "  0.14097449  0.05455104  0.05527564  0.04227823  0.18055181  0.85995294\n",
      "  0.93959826  0.13593849  0.09132107  0.06371962  0.0651597   0.07184875\n",
      "  0.04588333  0.04902909  0.03532797  0.06811228  0.96429838  0.87725625\n",
      "  0.86073068  0.93694988  0.1196077   0.02955629  0.76332237  0.89131885\n",
      "  0.97469369  0.73336604  0.10888415  0.04885625  0.82669997  0.28668193\n",
      "  0.0403756   0.02955162  0.08789331  0.0827087   0.91568697  0.52715107\n",
      "  0.04702581  0.03290225  0.07863151  0.070825    0.96180177  0.30321853\n",
      "  0.74903272  0.96811246  0.05463911  0.0651597   0.05094152  0.04461554\n",
      "  0.04467751  0.06986977  0.92943651  0.92310173  0.50878581  0.0575872\n",
      "  0.0651597   0.95988211  0.050465    0.06433903  0.06816065  0.07446222\n",
      "  0.070825    0.06778028  0.05136521  0.20630882  0.13121918  0.0613962\n",
      "  0.11821543  0.06835571  0.1719406   0.94514166  0.07664171  0.96626394\n",
      "  0.94153825  0.10607909  0.05241482  0.89361078  0.38471272  0.09081561\n",
      "  0.96974606  0.96126293  0.08774247  0.88755105  0.95555068  0.94314644\n",
      "  0.56343523  0.8423792   0.14511466  0.44135783  0.06599075  0.49601082\n",
      "  0.91757698  0.07591586  0.06376409  0.38998236  0.85152668  0.43870701\n",
      "  0.96022136  0.49601082  0.05442765  0.08831304  0.98378923  0.26729803\n",
      "  0.50119277  0.07157903  0.45283117  0.91705331  0.4581501   0.11024135\n",
      "  0.07575265  0.31346452  0.9821276   0.06239142  0.51395659]\n",
      "k [ 0.08581884  0.05882038  0.21343709  0.2075291   0.17905879  0.14770083\n",
      "  0.15796463  0.03829713  0.0375057   0.471818    0.67271455  0.10806172\n",
      "  0.1179122   0.07447895  0.8209214   0.13777844  0.82049349  0.17394228\n",
      "  0.1402368   0.11289645  0.13070785  0.61039653  0.8765314   0.5533354\n",
      "  0.82945853  0.70577504  0.44627797  0.30253068  0.21343818  0.32648776\n",
      "  0.21260152  0.18947299  0.05914491  0.88400682  0.07779497  0.07528027\n",
      "  0.07418381  0.0712222   0.5391768   0.3158776   0.78131462  0.74807571\n",
      "  0.04431077  0.10633496  0.09823445  0.13501662  0.13551113  0.90919848\n",
      "  0.8699103   0.1203364   0.16716728  0.41404547  0.8630323   0.08143188\n",
      "  0.07620926  0.10633496  0.09823445  0.11133085  0.0801424   0.07528027\n",
      "  0.07952621  0.0776408   0.04389395  0.33110151  0.19595899  0.11577643\n",
      "  0.19964406  0.07556569  0.34716052  0.85231466  0.1543811   0.9027588\n",
      "  0.88233866  0.14587754  0.10866621  0.82554676  0.26459929  0.16336335\n",
      "  0.89242133  0.80447874  0.31016463  0.77284039  0.87746136  0.7733703\n",
      "  0.44580728  0.67175366  0.29714799  0.3679619   0.04708835  0.32939265\n",
      "  0.86163407  0.13385106  0.1668094   0.21106548  0.69134029  0.40556081\n",
      "  0.92980903  0.32939265  0.04795457  0.19986576  0.93826857  0.33810499\n",
      "  0.39205314  0.11767849  0.2525972   0.77755105  0.41129586  0.18957719\n",
      "  0.10516508  0.36059841  0.95665582  0.11208967  0.36731659]\n",
      "[[[0.20277260987486218, 0.71569390126762089, 0.80471462851226438], [0.36956723033977779, 0.3298710235434289, 0.29113234500216578]], [[1.0059090277772591, 1.003225826377288, 1.0094423294776673], [0.98589804257045888, 0.98668291427968358, 0.98727312784725707]]]\n",
      "[20 50]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ikea/.local/lib/python2.7/site-packages/matplotlib/font_manager.py:1288: UserWarning: findfont: Font family [u'Arial'] not found. Falling back to Bitstream Vera Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc7016260d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAGMCAYAAAD0nYndAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIAjalv8Dvz9/TIg6trQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc6fea1bc90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import oneclasscurve  as occ\n",
    "sizes=[20,50]\n",
    "a,b = occ.get_results(repeats=3,sizes=sizes,njobs=3,argparam=-2)\n",
    "occ.plot('123',sizes,a,b)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import oneclasscurve  as occ\n",
    "occ.plot('123',sizes,a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a=np.array([[ 0.09634476 , 0.90365524],\n",
    " [ 0.06088005,  0.93911995],\n",
    " [ 0.07065094 , 0.92934906],\n",
    " [ 0.04671271,  0.95328729],\n",
    " [ 0.02970209 , 0.97029791],\n",
    " [ 0.29515478 , 0.70484522],\n",
    " [ 0.86338974 , 0.13661026],\n",
    " [ 0.80168559 , 0.19831441],\n",
    " [ 0.80968646 , 0.19031354],\n",
    " [ 0.12463197  ,0.87536803],\n",
    " [ 0.1517372   ,0.8482628 ],\n",
    " [ 0.34249768,  0.65750232],\n",
    " [ 0.07255634,  0.92744366],\n",
    " [ 0.02922214,  0.97077786],\n",
    " [ 0.77822639,  0.22177361],\n",
    " [ 0.81886221,  0.18113779],\n",
    " [ 0.16694517,  0.83305483],\n",
    " [ 0.12744298,  0.87255702],\n",
    " [ 0.09078483,  0.90921517],\n",
    " [ 0.04598973,  0.95401027],\n",
    " [ 0.80762083 , 0.19237917],\n",
    " [ 0.03582149 , 0.96417851],\n",
    " [ 0.82493818  ,0.17506182],\n",
    " [ 0.07749642 , 0.92250358],\n",
    " [ 0.52502072 , 0.47497928],\n",
    " [ 0.20059041 , 0.79940959],\n",
    " [ 0.54677554  ,0.45322446],\n",
    " [ 0.75185612  ,0.24814388],\n",
    " [ 0.72276445 , 0.27723555],\n",
    " [ 0.78464842 , 0.21535158],\n",
    " [ 0.41365199 , 0.58634801],\n",
    " [ 0.09839525 , 0.90160475],\n",
    " [ 0.03955736  ,0.96044264],\n",
    " [ 0.39204652  ,0.60795348],\n",
    " [ 0.58669915  ,0.41330085],\n",
    " [ 0.09951188  ,0.90048812],\n",
    " [ 0.69697943  ,0.30302057],\n",
    " [ 0.7775214   ,0.2224786 ]])\n",
    "\n",
    "print a[:,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

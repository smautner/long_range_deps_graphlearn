{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ikea/GraphLearn/GraphLearn_examples/Abstract\r\n"
     ]
    }
   ],
   "source": [
    "from eden.util import configure_logging\n",
    "import logging\n",
    "configure_logging(logging.getLogger(),verbosity=1)\n",
    "%matplotlib inline\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "GET RNA DATA\n",
    "'''\n",
    "from eden.converter.fasta import fasta_to_sequence\n",
    "import itertools\n",
    "from eden.util import random_bipartition_iter\n",
    "import random\n",
    "\n",
    "def rfam_uri(family_id):\n",
    "    return 'http://rfam.xfam.org/family/%s/alignment?acc=%s&format=fastau&download=0'%(family_id,family_id)\n",
    "def rfam_uri(family_id):\n",
    "    return '%s.fa'%(family_id)\n",
    " \n",
    "    \n",
    "    \n",
    "RFAM=\"mixed\"\n",
    "#cutoff 162 (44.0) \n",
    "#cutoff 1725 (38.0)\n",
    "#cutoff rest (29)\n",
    "\n",
    "\n",
    "def get_sequences(size=9999,rand=False):\n",
    "    sequences = get_sequences_with_names(size=size,rand=rand)\n",
    "    return [ b for (a,b) in sequences ]\n",
    "\n",
    "def get_sequences_with_names(size=9999, rand=False):\n",
    "    if rand:\n",
    "        sequences , boring = random_bipartition_iter(fasta_to_sequence(\"../toolsdata/%s.fa\" % RFAM),.9,random_state=random.random())\n",
    "        sequences = itertools.islice( sequences , size)\n",
    "    else:\n",
    "        sequences = itertools.islice( fasta_to_sequence(\"../toolsdata/%s.fa\" % RFAM), size)\n",
    "    return sequences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-498f921ff750>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mpp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrna\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPreProcessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore_inserts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mpp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_sequences_with_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mgraphmanagers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ikea/GraphLearn/graphlearn/abstract_graphs/RNA.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, inputs, vectorizer)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNNmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEdenNNF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNNmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ikea/GraphLearn/graphlearn/abstract_graphs/RNA.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, sequencelist)\u001b[0m\n\u001b[0;32m    496\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequencelist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meden_rna_vectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meden\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRNA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meden_rna_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequencelist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;31m# after the initial thing: settting min enery high so we never do mfe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ikea/EDeN/eden/RNA.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, seqs)\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;31m# store seqs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseqs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormalize_seqs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseqs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[0mdata_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseqs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m         \u001b[1;31m# fit nearest_neighbors model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnearest_neighbors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ikea/EDeN/eden/path.pyc\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, seq_list)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mfeature_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0minstance_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m             \u001b[0mfeature_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_dict_to_sparse_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ikea/EDeN/eden/path.pyc\u001b[0m in \u001b[0;36m_transform\u001b[1;34m(self, instance_id, seq)\u001b[0m\n\u001b[0;32m     94\u001b[0m                                                        \u001b[0mradius\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m                                                        \u001b[0mdistance\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m                                                        \u001b[0mneighborhood_hash_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msecond_endpoint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mradius\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m                                                        self.bitmask)\n\u001b[0;32m     98\u001b[0m                             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfast_hash_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mradius\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbitmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''TESTING EXTRACTION AND GRAPHMANAGER'''\n",
    "from graphlearn.utils import draw\n",
    "import graphlearn.abstract_graphs.RNA as rna\n",
    "from graphlearn.graphlearn import Sampler as GLS\n",
    "from eden.graph import Vectorizer\n",
    "\n",
    "vectorizer=Vectorizer()\n",
    "pp=rna.PreProcessor(ignore_inserts=True)\n",
    "pp.fit(get_sequences_with_names(),vectorizer)\n",
    "graphmanagers=pp.transform(get_sequences()[:20])\n",
    "\n",
    "print 'DEMONSTRATING GRAPH MANAGER'\n",
    "\n",
    "for i in [4]:\n",
    "    print 'grammar example %d' % i\n",
    "    gm=graphmanagers[i]\n",
    "    g=gm.graph(nested=True)\n",
    "    #print g.nodes(data=True)\n",
    "    #g.node[0].pop('weight')\n",
    "    # jump because we do the thesis now\n",
    "    #vec=vectorizer.transform_single(g)\n",
    "    draw.graphlearn([gm.graph(nested=True),gm.abstract_graph(),gm.base_graph()], size = 15,vertex_label = 'label',contract=False)\n",
    "    draw.graphlearn([gm.base_graph()], size = 15,vertex_label = 'label',contract=True)\n",
    "    \n",
    "    print gm.sequence\n",
    "    print gm.structure\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "'''\n",
    "print 'DEMONSTRATING EXTRACTION'  \n",
    "#prepare\n",
    "radius_list=[0,2]\n",
    "thickness_list=[2,4]\n",
    "base_thickness_list=[2]\n",
    "f=lambda x, y: True\n",
    "d={'radius_list':radius_list,\n",
    "    'thickness_list':thickness_list,\n",
    "    'hash_bitmask':2**20-1,\n",
    "    'node_filter': f}\n",
    "\n",
    "\n",
    "#draw.graphlearn([gm.abstract_graph(),gm.base_graph()], size = 15,vertex_label = 'id',contract=True)\n",
    "cips=gm.all_core_interface_pairs(**d)\n",
    "for li, ciplist in enumerate(cips):\n",
    "    for entry, cip in enumerate(ciplist):\n",
    "        continue\n",
    "        z='list: %d ; entry: %d' % (li,entry)\n",
    "        #cip.graph.graph['info']=z\n",
    "        draw.graphlearn([cip.graph,cip.abstract_view], size=4, contract=True) \n",
    "cipz=[cips[2][3],cips[1][3],cips[1][2]]\n",
    "graphz=[[c.graph,c.abstract_view] for c in cipz]\n",
    "    \n",
    "#g=gm.graph(nested=False)\n",
    "#print 'test', g.nodes(data=True)[0][1]\n",
    "\n",
    "#for graphpair in graphz:\n",
    "#    draw.graphlearn(graphpair, size=10,font_size=36,# secondary_vertex_label='ID',\n",
    "#                   colormap='Paired', invert_colormap=False,node_border=0.5,contract=True,\n",
    "#                   vertex_alpha=0.5, node_size=1900)\n",
    "    \n",
    "    \n",
    "    \n",
    "minor = gm.abstract_graph()\n",
    "minorcip = graphz[0][1]\n",
    "\n",
    "def wire(g,nset=[]):\n",
    "    for n in nset:\n",
    "        z=str(n)\n",
    "        g.add_edge(\"+\"+z,\"-\"+z,nesting=True,weight='.2',len=2)\n",
    "        \n",
    "draw.graphlearn([minor,minorcip], size=10,font_size=36,# secondary_vertex_label='ID',\n",
    "                   colormap='Paired', invert_colormap=False,node_border=0.5,contract=True,\n",
    "                   vertex_alpha=0.5, node_size=1900)\n",
    "#[minor,minorcip]\n",
    "\n",
    "def colorize(g,c1=[],c2=[],c3=[],label='mycolors'):\n",
    "    for n,d in g.nodes(data=True):\n",
    "        if n in c1:\n",
    "            d[label]='#C88A68' #reddish\n",
    "        elif n in c2: \n",
    "            d[label]='#FAF4B0' # yellow\n",
    "        elif n in c3:\n",
    "            d[label]='#C9BC24' # dark yellow\n",
    "        else:\n",
    "            d[label]='#C1DDEB' # default color -- blueish\n",
    "\n",
    "colorize(minor,c1=[2,3],c2=[5,9,10,0],c3=[])\n",
    "colorize(minorcip,c1=[2,3],c2=[5,9,10,0],c3=[])\n",
    "#g3=draw.cip_to_drawable_graph(cips=[cips[2][3]], graphs=[minor])\n",
    "\n",
    "draw.graphlearn([minor,minorcip], size=10,font_size=36, #secondary_vertex_label='id',\n",
    "                    contract=True,\n",
    "                   colormap='Paired', invert_colormap=False,node_border=0.5,vertex_color='mycolors',\n",
    "                   vertex_alpha=0.5, node_size=1900)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "'''\n",
    "learning a grammar\n",
    "'''\n",
    "import graphlearn.abstract_graphs.RNA as rna\n",
    "from  graphlearn.feasibility import FeasibilityChecker as Checker\n",
    "import graphlearn.estimator as estimator\n",
    "# not really needed since after refolding we get an RNA\n",
    "#feasibility=Checker()\n",
    "#feasibility.checklist.append(rna.is_rna)\n",
    "graphs = get_sequences_with_names(size=200,rand=True)\n",
    "graphs = list(graphs)\n",
    "\n",
    "\n",
    "estimator=estimator.Wrapper( nu=.33, cv=2, n_jobs=-1)\n",
    "\n",
    "sampler=rna.AbstractSampler(radius_list=[0,1],\n",
    "                            thickness_list=[2], \n",
    "                            min_cip_count=1, \n",
    "                            min_interface_count=2, \n",
    "                            preprocessor=rna.PreProcessor(base_thickness_list=[1],ignore_inserts=True), \n",
    "                            postprocessor=rna.PostProcessor(),\n",
    "                            estimator=estimator\n",
    "                            #feasibility_checker=feasibility\n",
    "                           )\n",
    "sampler.fit(graphs,grammar_n_jobs=1,grammar_batch_size=1)\n",
    "sampler.save('../tmp/rna_ubergrammar.ge')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#draw production rules\n",
    "draw.draw_grammar(sampler.lsgg.productions,n_productions=5,n_graphs_per_production=5,\n",
    "                     n_graphs_per_line=6, size=10, contract=False,\n",
    "                     colormap='Paired', invert_colormap=False,node_border=1,\n",
    "                     vertex_alpha=0.6, edge_alpha=0.5, node_size=250, abstract_interface=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "'''\n",
    "Rna sampling\n",
    "'''\n",
    "import os\n",
    "os.nice(19)\n",
    "import graphlearn.utils.draw as draw\n",
    "import graphlearn.abstract_graphs.RNA as rna\n",
    "from graphlearn.graphlearn import Sampler as GLS\n",
    "import itertools\n",
    "#sampler=rna.AbstractSampler(radius_list=[0,1],thickness_list=[2], min_cip_count=1, min_interface_count=2, preprocessor=rna.PreProcessor(), postprocessor=rna.PostProcessor())\n",
    "#sampler.load('tmp/rna_ubergrammar.ge')\n",
    "\n",
    "\n",
    "#graphs = get_sequences()\n",
    "graphs = [ b for a ,b in graphs  ]\n",
    "id_start=66\n",
    "id_end=id_start+12\n",
    "graphs = itertools.islice(graphs,id_start,id_end)\n",
    "n_steps=50\n",
    "\n",
    "graphs = sampler.sample(graphs,\n",
    "                        n_samples=3,\n",
    "                        batch_size=1,\n",
    "                        n_steps=n_steps,\n",
    "                        n_jobs=4,\n",
    "                        quick_skip_orig_cip=True,\n",
    "                        probabilistic_core_choice=True,\n",
    "                        burnin=0,\n",
    "                        improving_threshold=0.9,\n",
    "                        improving_linear_start=0.3,\n",
    "                        max_size_diff=20,\n",
    "                        accept_min_similarity=0.65,\n",
    "                        select_cip_max_tries=30,\n",
    "                        keep_duplicates=False,\n",
    "                        include_seed=True,\n",
    "                        backtrack=10,\n",
    "                        monitor=True)\n",
    "\n",
    "\n",
    "from eden.modifier.graph.vertex_attributes import colorize   \n",
    "scores=[]\n",
    "sequences=[]\n",
    "ids=range(id_start,id_end)\n",
    "for i,graphlist in enumerate(graphs):\n",
    "    print 'Graph id: %d'%(ids[i])\n",
    "    scores.append(sampler.monitors[i].sampling_info['score_history'])\n",
    "    \n",
    "    sequences.append(sampler.monitors[i].sampling_info['notes'])\n",
    "    \n",
    "    path_graphs = colorize(graphlist,\n",
    "                           output_attribute = 'color_level', \n",
    "                           labels = ['A','U','G','C'])\n",
    "    path_graphs= list(path_graphs)\n",
    "    #draw.graphlearn(path_graphs,\n",
    "    #                       n_graphs_per_line=3, size=20, \n",
    "    #                       colormap='Paired', invert_colormap=False,node_border=0.5, vertex_color='color_level',\n",
    "    #                       vertex_alpha=0.5, edge_alpha=0.7, node_size=450,edge_label='label' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "%matplotlib inline\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "step=1\n",
    "num_graphs_per_plot=3\n",
    "num_plots=np.ceil([len(scores)/num_graphs_per_plot])\n",
    "for i in range(num_plots):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    for j,score in enumerate(scores[i*num_graphs_per_plot:i*num_graphs_per_plot+num_graphs_per_plot]):\n",
    "        data = list(islice(score,None, None, step))\n",
    "        plt.plot(data, label='graph %d'%(j+i*num_graphs_per_plot+id_start))\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid()\n",
    "    plt.ylim(-0.1,1.1)\n",
    "    plt.show()\n",
    "    \n",
    "'''\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "draw the score history for each of the graphs\n",
    "'''\n",
    "colors=['b','g','r','c','m','y','k','w']\n",
    "\n",
    "%matplotlib inline\n",
    "from itertools import islice\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "step=1\n",
    "num_graphs_per_plot=2\n",
    "num_plots=np.ceil([len(scores)/num_graphs_per_plot])\n",
    "for i in range(num_plots):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    for j,score in enumerate(scores[i*num_graphs_per_plot:i*num_graphs_per_plot+num_graphs_per_plot]):\n",
    "        data = list(islice(score,None, None, step))\n",
    "        plt.plot(data,ls='-',color=colors[j], label='graph %d'%(j+i*num_graphs_per_plot+id_start))\n",
    "        '''\n",
    "        # okok now we need to add the infernal evaluation\n",
    "        seqs=sequences[i*num_graphs_per_plot+j]\n",
    "        seqs=seqs.split('n')\n",
    "        \n",
    "        # SEQUENCES STILL CONTAIN F, ALSO THERE ARE ERRORMESSAGES AT THE END OF INFO\n",
    "        #seqs=seqs[:-1]\n",
    "    \n",
    "        \n",
    "        data2= rna.infernal_checker(seqs,cmfile='../RF00162.cm')\n",
    "        plt.plot(data2,ls='--',color=colors[j], label='infernal %d'%(j+i*num_graphs_per_plot+id_start))\n",
    "        '''\n",
    "    plt.plot([0.29]*len(data2),ls='-.',color='r', label='significance') #| '-' | '--' | '-.' | ':' | 'None' | ' ' |\n",
    "    plt.legend(loc='lower left',framealpha=0.5)\n",
    "    plt.grid()\n",
    "    plt.ylim(-0.1,1.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import graphlearn.abstract_graphs.RNA as rna\n",
    "import graphlearn.utils.draw as draw\n",
    "\n",
    "#faillist=[0,2,4,5]\n",
    "# for every failed graph\n",
    "\n",
    "runs=[]\n",
    "for i,mon in enumerate(sampler.monitors):\n",
    "    lastgraph=None\n",
    "    run=[]\n",
    "    for t,graphs in mon.format(start=0):\n",
    "        if lastgraph!=None:\n",
    "            v=sampler.vectorizer.similarity([graphs[0].copy()],lastgraph).next()\n",
    "            \n",
    "            run.append(v)\n",
    "        lastgraph=graphs[0]\n",
    "    runs.append(run)\n",
    "import numpy\n",
    "for run in runs:\n",
    "    a= numpy.array(run)\n",
    "    print numpy.mean(a, axis=0),' ',numpy.std(a, axis=0)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    if len(mon.content) < 80:\n",
    "        for t,graphs in mon.format(start=0):\n",
    "            # print relevant info\n",
    "\n",
    "            draw.graphlearn(graphs,size=6,node_border=.5)\n",
    "\n",
    "            lastgraph=graphs[0]\n",
    "            \n",
    "        print \"#\"*80\n",
    "        print \"#\"*80\n",
    "        break\n",
    "            #print monentry['graphwrapper'].sequence\n",
    "            #print monentry['graphwrapper'].structure\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "In [18]: arr =\n",
    "\n",
    "In [20]: numpy.mean(arr, axis=0)\n",
    "Out[20]: \n",
    "array([ 0.7       ,  2.2       ,  1.8       ,  2.13333333,  3.36666667,\n",
    "        5.1       ])\n",
    "\n",
    "In [21]: numpy.std(arr, axis=0)\n",
    "Out[21]: \n",
    "array([ 0.45460606,  1.29614814,  1.37355985,  1.50628314,  1.15566239,\n",
    "        1.2083046 ])\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print run\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import networkx as nx\n",
    "import graphlearn.utils.draw as draw\n",
    "from eden.graph import Vectorizer\n",
    "\n",
    "v=Vectorizer()\n",
    "def getpathgraph(labels):\n",
    "    nlabels= len(labels)\n",
    "    G=nx.path_graph(nlabels)\n",
    "    for e in range(nlabels):\n",
    "        G.node[e]['label']=labels[e]\n",
    "    for e in range(nlabels-1):\n",
    "        G.edge[e][e+1]['label']='.'\n",
    "    return G\n",
    "\n",
    "g=getpathgraph(\"ABC\")\n",
    "\n",
    "g= v._edge_to_vertex_transform(g)\n",
    "\n",
    "draw.graphlearn_draw(g,contract=False,show_direction=True)\n",
    "\n",
    "g2=nx.DiGraph(g)\n",
    "#draw.display(g2,contract=False,vertex_label='id')\n",
    "\n",
    "for n,d in g2.nodes(data=True):\n",
    "    if 'edge' in d:\n",
    "        if d['label']=='.':\n",
    "            ns=g2.neighbors(n)\n",
    "            g2.remove_edge(ns[1],n)\n",
    "            g2.remove_edge(n,ns[0])\n",
    "          \n",
    "       \n",
    "#g2 = g2.to_undirected()\n",
    "draw.graphlearn(g2,contract=False,vertex_label='id',show_direction=True)\n",
    "g2.edges()\n",
    "for n in g2.nodes():\n",
    "    print n,g2.neighbors(n)\n",
    "print isinstance(g2, nx.DiGraph) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g2.edges(0,data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "asd = g2.subgraph(range(2)).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class A(object):\n",
    "    def __init__(self,a=123):\n",
    "        self.a=a\n",
    "        \n",
    "        \n",
    "class B(A):\n",
    "    def __init__(self,b=122):\n",
    "        super(B,self).__init__(b)\n",
    "        \n",
    "        \n",
    "\n",
    "class C(A):\n",
    "    def __init__(self,c=121):\n",
    "        super(C,self).__init__(c)\n",
    "        \n",
    "        \n",
    "c=C()\n",
    "c.a\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# testin cycle stuff\n",
    "import networkx as nx\n",
    "import graphlearn.utils.draw as draw\n",
    "import graphlearn.feasibility as feas\n",
    "from eden.graph import Vectorizer\n",
    "v=Vectorizer()\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def find_all_cycles(G, source=None, cycle_length_limit=None):\n",
    "    \"\"\"forked from networkx dfs_edges function. Assumes nodes are integers, or at least\n",
    "    types which work with min() and > .\"\"\"\n",
    "    if source is None:\n",
    "        # produce edges for all components\n",
    "        nodes=[i[0] for i in nx.connected_components(G)]\n",
    "    else:\n",
    "        # produce edges for components with source\n",
    "        nodes=[source]\n",
    "    # extra variables for cycle detection:\n",
    "    cycle_stack = []\n",
    "    output_cycles = set()\n",
    "    \n",
    "    def get_hashable_cycle(cycle):\n",
    "        \"\"\"cycle as a tuple in a deterministic order.\"\"\"\n",
    "        m = min(cycle)\n",
    "        mi = cycle.index(m)\n",
    "        mi_plus_1 = mi + 1 if mi < len(cycle) - 1 else 0\n",
    "        if cycle[mi-1] > cycle[mi_plus_1]:\n",
    "            result = cycle[mi:] + cycle[:mi]\n",
    "        else:\n",
    "            result = list(reversed(cycle[:mi_plus_1])) + list(reversed(cycle[mi_plus_1:]))\n",
    "        return tuple(result)\n",
    "    \n",
    "    for start in nodes:\n",
    "        if start in cycle_stack:\n",
    "            continue\n",
    "        cycle_stack.append(start)\n",
    "        \n",
    "        stack = [(start,iter(G[start]))]\n",
    "        while stack:\n",
    "            parent,children = stack[-1]\n",
    "            try:\n",
    "                child = next(children)\n",
    "                \n",
    "                if child not in cycle_stack:\n",
    "                    cycle_stack.append(child)\n",
    "                    stack.append((child,iter(G[child])))\n",
    "                else:\n",
    "                    i = cycle_stack.index(child)\n",
    "                    if i < len(cycle_stack) - 2: \n",
    "                      output_cycles.add(get_hashable_cycle(cycle_stack[i:]))\n",
    "                \n",
    "            except StopIteration:\n",
    "                stack.pop()\n",
    "                cycle_stack.pop()\n",
    "    \n",
    "    return [list(i) for i in output_cycles]\n",
    "\n",
    "\n",
    "def getpathgraph(labels):\n",
    "    nlabels= len(labels)\n",
    "    G=nx.path_graph(nlabels)\n",
    "    for e in range(nlabels):\n",
    "        G.node[e]['label']=labels[e]\n",
    "    for e in range(nlabels-1):\n",
    "        G.edge[e][e+1]['label']='.'\n",
    "    return G\n",
    "\n",
    "#g=getpathgraph(\"ABC\")\n",
    "\n",
    "#g= v._edge_to_vertex_transform(g)\n",
    "\n",
    "#draw.graphlearn_draw(g,contract=False,show_direction=True)\n",
    "\n",
    "\n",
    "#draw.display(g2,contract=False,vertex_label='id')\n",
    "\n",
    "g=nx.cycle_graph(10)\n",
    "g.add_edge(2,7)\n",
    "g.add_edge(10,0)\n",
    "\n",
    "draw.graphlearn_draw(g, vertex_label='id')\n",
    "\n",
    "\n",
    "feas.rooted_problem_cycle(g,2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print g.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stru,stri='(.((...))...)', 'aFaaaaaaaaaaa'\n",
    "\n",
    "\n",
    "\n",
    "def pairs(s):\n",
    "    \"give me a bond dict\"\n",
    "    unpaired=[]\n",
    "    pairs={}\n",
    "    for i,c in enumerate(s):\n",
    "        if c=='(':\n",
    "            unpaired.append(i)\n",
    "        if c==')':\n",
    "            partner=unpaired.pop()\n",
    "            pairs[i]=partner\n",
    "            pairs[partner]=i\n",
    "    return pairs\n",
    "\n",
    "\n",
    "\n",
    "def fix_structure( stru,stri ):\n",
    "    '''\n",
    "    the problem is to check every (( and )) . \n",
    "    if the bonding partners are not next to each other we know that we need to act.\n",
    "    '''\n",
    "    p=pairs(stru)\n",
    "    lastchar=\".\"\n",
    "    problems=[]\n",
    "    for i,c in enumerate(stru):\n",
    "        # checking for )) and ((\n",
    "        if c==lastchar and c!='.':\n",
    "            if abs(p[i]-p[i-1])!=1: #the partners are not next to each other\n",
    "                problems.append(i)\n",
    "        # )( provlem\n",
    "        elif c=='(':\n",
    "            if lastchar==')':\n",
    "                problems.append(i)          \n",
    "        lastchar=c\n",
    "            \n",
    "    problems.sort(reverse=True)\n",
    "    for i in problems:\n",
    "        stru=stru[:i]+'.'+stru[i:]\n",
    "        stri=stri[:i]+'F'+stri[i:]\n",
    "\n",
    "    return stru,stri\n",
    "    \n",
    "\n",
    "    \n",
    "fix_structure(stru,stri)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l=[[4,5],[3,4]]\n",
    "l.sort()\n",
    "l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import graphlearn.abstract_graphs.RnaWrap as RW\n",
    "from eden.converter.fasta import fasta_to_sequence   \n",
    "seq = [ b for (a,b) in fasta_to_sequence(\"RF00005.fa\") ] \n",
    "        \n",
    "folder=RW.NearestNeighborFolding(seq,4)\n",
    "\n",
    "folder.fold(\"GGGGCCUUAGCUCAGCUGGGAGAGCGCCUGCUUUGCACGCAGGAGGUCAGCGGUUCGAUGGCGCUAGGCUCCA\")\n",
    "a,b=folder.call_folder()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "textwrap.wrap(\"asdasdasdasdasdasd\", width=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s=\"asdasdasdasd\"\n",
    "s.get(z,123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z=[1,2,3]\n",
    "z.reverse()\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lol():\n",
    "    print \"a\"\n",
    "    return False\n",
    "\n",
    "if True or lol():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import networkx as nx\n",
    "import eden.modifier.rna.lib_forgi as lib_forgi\n",
    "\n",
    "\n",
    "\n",
    "def make_abstract_graph(forgi):\n",
    "    g=forgi_to_graph(forgi)\n",
    "    connect_multiloop(g)\n",
    "    return g\n",
    "\n",
    "\n",
    "def forgi_to_graph(forgi, ignore_inserts=True):\n",
    "    def make_node_set(numbers):\n",
    "        '''\n",
    "        forgi gives me stuff like define STEM START,END,START,END .. we take indices and output a list\n",
    "        '''\n",
    "        numbers=map(int,numbers)\n",
    "        ans=set()\n",
    "        while len(numbers)>1:\n",
    "            a,b = numbers[:2]\n",
    "            numbers=numbers[2:]\n",
    "            for n in range(a-1,b): ans.add(n) # should be range a,b+1 but the biologists are weired\n",
    "        return ans\n",
    "\n",
    "\n",
    "    def get_pairs(things):\n",
    "        '''\n",
    "        '''\n",
    "        current=[]\n",
    "        for thing in things:\n",
    "            if thing[0]=='m':\n",
    "                current.append(thing)\n",
    "            if len(current)==2:\n",
    "                yield current\n",
    "                current = []\n",
    "\n",
    "\n",
    "    g=nx.Graph()\n",
    "    fni={} # forgi name to networkx node id\n",
    "\n",
    "    for l in forgi.split('\\n')[:-1]:\n",
    "        line= l.split()\n",
    "        #only look at interesting lines\n",
    "        if line[0] not in ['define','connect']:\n",
    "            continue\n",
    "\n",
    "        # parse stuff like: define s0 1 7 65 71\n",
    "        if line[0]=='define':\n",
    "\n",
    "            # get necessary attributes for a node\n",
    "            label=line[1][0]\n",
    "            id=line[1]\n",
    "            myset=make_node_set(line[2:])\n",
    "            node_id=len(g)\n",
    "\n",
    "            # build a node and remember its id\n",
    "            g.add_node(node_id)\n",
    "            fni[id]=node_id\n",
    "            g.node[node_id].update( {'label':label, 'contracted':myset}  )\n",
    "\n",
    "        # parse stuff like this: connect s3 h2 m1 m3\n",
    "        if line[0]=='connect':\n",
    "            # get nx name of the first element.\n",
    "            hero= fni[ line[1] ]\n",
    "            # connect that hero to the following elements\n",
    "            for fn in line[2:]:\n",
    "                g.add_edge(hero, fni[fn])\n",
    "\n",
    "            # remember what pairs multiloop pieces we are part of\n",
    "            # i assume that if a stack is part of 2 multiloops they appear in order ..\n",
    "            # this assumption may be wrong so be careful\n",
    "            g.node[fni[line[1]]]['multipairs']=[]\n",
    "            for a,b in get_pairs(line[2:]):\n",
    "                g.node[fni[line[1]]]['multipairs'].append( (fni[a],fni[b]) )\n",
    "    if ignore_inserts:\n",
    "        # repair inserts by mergind adjacent stacks\n",
    "        mergelist=[]\n",
    "        for n,d in g.nodes(data=True):\n",
    "            if d['label']==\"i\":\n",
    "                neighs=g.neighbors(n)\n",
    "                mergelist.append((neighs[0],(n,neighs[1])))\n",
    "                \n",
    "        # merged is keeping track of already merged nodes.\n",
    "        # so we always know where to look for :) \n",
    "        merged={}\n",
    "        for s1,mergers in mergelist:\n",
    "            while s1 not in g.nodes():\n",
    "                s1=merged[s1]\n",
    "            for merger in mergers:\n",
    "                while merger not in g.nodes():\n",
    "                    merger=merged[merger]\n",
    "                merge(g,s1,merger)\n",
    "                merged[merger]=s1\n",
    "    return g\n",
    "\n",
    "def merge(graph, node, node2):\n",
    "    '''\n",
    "    merge node2 into the node.\n",
    "    input nodes are strings,\n",
    "    node is the king\n",
    "    '''\n",
    "    for n in graph.neighbors(node2):\n",
    "        graph.add_edge(node, n)\n",
    "    graph.node[node]['contracted'].update(graph.node[node2]['contracted'])\n",
    "    graph.remove_node(node2)\n",
    "\n",
    "\n",
    "def connect_multiloop(g):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    merge_dict={}\n",
    "    for node,d in g.nodes(data=True):\n",
    "        if d['label'] == 's':\n",
    "            for a,b in g.node[node]['multipairs']:\n",
    "                # finding real names... this works by walking up the\n",
    "                #ladder merge history until the root is found :)\n",
    "                while a not in g:\n",
    "                    a=merge_dict[a]\n",
    "                while b not in g:\n",
    "                    b=merge_dict[b]\n",
    "                if a==b:\n",
    "                    continue\n",
    "                merge_dict[b]=a\n",
    "                merge(g,a,b)\n",
    "\n",
    "s=\"((((((.(((((.((.......)).))))).............((((((...)))))).(((((.......))))).)))))).\"\n",
    "bg = lib_forgi.BulgeGraph()\n",
    "bg.from_dotbracket(s, None)\n",
    "forgi = bg.to_bg_string()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def transform( original_graph):\n",
    "        \"\"\"Return a graph where the edges of the original_graph are converted to nodes.\"\"\"\n",
    "\n",
    "        # if operating on graphs that have already been subject to the\n",
    "        # edge_to_vertex transformation, then do not repeat the transformation but\n",
    "        # simply return the graph\n",
    "        if 'expanded' in original_graph.graph:\n",
    "            return original_graph\n",
    "        else:\n",
    "            graph = nx.Graph()\n",
    "            graph.graph['expanded'] = True\n",
    "            # build a graph that has as vertices the original vertex set\n",
    "            for n, d in original_graph.nodes_iter(data=True):\n",
    "                d['node'] = True\n",
    "                graph.add_node(n, d)\n",
    "            # and in addition a vertex for each edge\n",
    "            new_node_id = max(original_graph.nodes()) + 1\n",
    "            for u, v, d in original_graph.edges_iter(data=True):\n",
    "                d['edge'] = True\n",
    "                \n",
    "                graph.add_node(new_node_id, d)\n",
    "                print u,v,new_node_id\n",
    "                # and the corresponding edges\n",
    "                graph.add_edge(new_node_id, u, label=None)\n",
    "                graph.add_edge(new_node_id, v, label=None)\n",
    "                \n",
    "                new_node_id += 1\n",
    "            return graph\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "g=forgi_to_graph(forgi)\n",
    "draw.graphlearn(g,contract=False,secondary_vertex_label='id')\n",
    "\n",
    "if 2 in g[2]:\n",
    "    g.remove_edge(2,2)\n",
    "g = transform(g)\n",
    "draw.graphlearn(g,contract=False,secondary_vertex_label='id',size=20)\n",
    "#self._abstract_graph = forgi.edge_parent_finder(g, self._base_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rfam_uri(family_id):\n",
    "    return 'http://rfam.xfam.org/family/%s/alignment?acc=%s&format=fastau&download=0'%(family_id,family_id)\n",
    "print rfam_uri(\"RF00162\")\n",
    "print rfam_uri(\"RF01725\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

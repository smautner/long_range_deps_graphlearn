\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib
% \usepackage[nonatbib]{nips_2016}

\usepackage{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
%\usepackage[final]{nips_2016}


\usepackage[english, status=draft]{fixme} % these 2 make fixme comments :) 
\fxusetheme{color}                        %

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}       %PICTSCHA
\usepackage{amsmath}        %METH, ok appears to be included in the amsfontz


\newcommand*{\IN}[0]{\ensuremath{\mathbb{N}}}



%%%%%%%%%%
% citation stuff
%%%%%%%%

\usepackage{natbib} 
%bibstyle muss einer da sein, 
% setimmt wie der kram an \cite aussieht.
% https://de.wikibooks.org/wiki/LaTeX-W%C3%B6rterbuch:_bibliographystyle
%https://de.sharelatex.com/learn/Bibtex_bibliography_styles
%\bibliographystyle{abbrvnat} 
%\bibliographystyle{unsrt} 
\bibliographystyle{plainnat}









\title{A constructive approach for graph concepts\\with long range dependencies}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Fabrizio Costa \\
  Department of Computer Science\\
  Albert-Ludwigs University Freiburg\\
  Freiburg, 79085  \\
  \texttt{costa@informatik.uni-freiburg.de} \\
  \And
  Stefan Mautner\\
  Department of Computer Science\\
  Albert-Ludwigs University Freiburg\\
  Freiburg, 79085  \\
  \texttt{mautner@cs.uni-freiburg.de} \\
  %% examples of more authors
  %%\AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
%Scripts describing the generation of generic graphs based on examples
%are scarce.

Machine learning constructive approaches offer a way to answer interesting
'design' questions on the basis of a collection of examples.  In particular,
graph constructive methods are of interest in chemo- and bio-informatics
domains where the task is to synthesize novel molecules with a desired
bioactivity. Most molecules, however, exhibit complex dependencies between
their different constituent parts. RNA polymers, for example, self interact,
with nucleotides forming bounds that can typically span the entire length of
the sequence. Since modeling long range dependencies is a difficult problem,
we propose an efficient solution, based on graph coarsening that builds on top
of a recent constructive approach and we show encouraging experimental results
on a RNA synthesis task.

% Graphs are flexible data structures whose generation 
% is a mostly unexplored problem. As discriminative systems for graphs
% exist, it is feasible to direct a generative process.
% An existing method is implementing Markov Chain Monte Carlo 
% sampling which is altering graphs incrementally.
% Alteration is done with a \emph{graph grammar}, a collection
% of sub graphs, grouped by exchangeability.

% \fxwarning{so the abstract was rewritten, i made sure to explain the contraction}
% By contracting edges in a graph i.e. merging adjacent vertices,
% we obtain a more abstract view on a graph. 
% We use this abstraction to overcome the shortcomings of the original 
% grammar.
% However, the modification is required to be automatically generate. 
% We explore the application of this idea
% on graphs representing RNA secondary structure. 



\end{abstract}
\section{Introduction}

Constructive machine learning addresses the problem of automatically 'build'
or 'design' artifacts given a concept expressed as a representative set of
examples. The task becomes particularly challenging in a discrete setting when
the solution space is exponential and direct enumeration approaches become
unfeasible, for example in the domain of graphs with discrete labels. In
\cite{costa16} the problem of generating elements of a structured domain was
framed as the equivalent problem of sampling from a corresponding underlying
probability distribution defined over a (learned) class of structures.
Approaches like {\em rejection sampling} where instances are generated without
enforcing constraints, but are then rejected if violations in the probability
distribution occur, are inefficient as the sampling space almost never yields
valid samples. In order to work with objects that have a non negligible
probability of being accepted they use a graph grammar. Specifically they
employ a context-sensitive grammar to accurately model complex dependencies
between different parts of an instance. The authors acknowledge that an
approach based exclusively on a grammar is not sufficient since the number of
proposed graphs grows exponentially with the number of production rules in the
grammar. In order to select only the most promising candidates one needs to
equip the grammar with a probabilistic notion. However any procedure to
estimate the probabilities of a context-sensitive grammar needs to trade off
the number of contextual rules with the expressiveness of the grammar. The
larger the context, the (exponentially) larger the number of possible cases
that need to be observed for a reliable estimate, which in turn requires an
exponentially large number of training examples. For this reason global or
just long range constraints cannot be effectively modeled, resulting in a
grammar that could express non viable instances. To address the issue
\cite{costa16} proposes to use a Metropolis Hastings (MH) Markov Chain Monte
Carlo (MCMC) method, where the problem of sampling is reduced to the easier
task of {\em simulation}, provided that one can build an ergodic Markov chain
with the desired distribution as its equilibrium distribution. They use the
context sensitive graph grammar to inform the MH proposal distribution, but
also introduce a probability density estimator to define the MH acceptance
procedure. This allows to deal separately with local and global constraints:
the locally context-sensitive graph grammar is used for the local constraints
and the regularized statistical model is used for the global or long range
constraints. The two approaches complement each other: the grammar is a
flexible non-parametric approach that can model complex dependencies between
vertices that are within a short path distance from each other; the
statistical model, instead, can employ the bias derived from the particular
functional family (linear) and the type of regularization used (a penalty over
the $\ell_2$ norm of the parameters) to generalize long range dependencies to
similar cases.

This approach is therefore adequate when the underlying concept exhibit local
dependencies that are more complex than long range ones.  Unfortunately in
some application domains instances can exhibit complex long range dependencies
between their different constituent parts. This is the case for RNA polymers,
long sequences of atomic entities (nucleotides) that self interact,
establishing pairwise bounds that can typically span the entire length of the
sequence.

A different way to view the issue of long range dependencies is that of the
appropriate scale of representation. Certain application domains exhibit
natural encodings, i.e. instances are encoded as graphs where nodes represent
specific entities, such as nucleotides in the case of RNA sequences. However,
it is known that a more effective functional description of RNA polymers can
be obtained in terms of structural components such as {\em stems} (stretches
of consecutive paired nucleotides) and {\em loops} (stretches of consecutive
unpaired nucleotides). Under this view, dependencies that are local at the
coarser scales correspond to longer range dependencies at the original scale.

A constructive system suitable for these domains needs to be able to
adequately model complex long range dependencies and is more effective if it
operates at a convenient coarser scale rather than that of the individual
units. Here we tackle all these issues building on the approach presented in
\cite{costa16} introducing two key ideas: 1) we allow a user/domain defined graph
coarsening procedure,
and 2) we allow domain specific optimization procedures to ensure that
generated instances are always viable.


\section{Method}

%summary of the approach: sampling using MH with grammar.
 
%what is a graph grammar. our grammar. CIPs.

%the extension.

\cite{costa16} presented an approach to sample graphs from an empirical
probability distribution, using a variant of the Metropolis Hastings (MH)
algorithm. The MH approach decomposes the transition probability of an
underlying  Markov process in two factors, 1) the proposal  and 2) the
acceptance-rejection probability distributions. The key element for an
efficient MH design consists in shaping the proposal distribution  in such a
way that the generated elements will not be rejected too often. To do so Costa
suggests to use a grammar and infer its rules from the available data using
grammatical inference techniques upgraded to structured domains (i.e. domains
where instances are naturally represented as labeled graphs).

More precisely, a graph grammar \citep{rozenberg1999handbook} is a finite set
of productions; a production is a triple $S=(M,D,E)$ where $M$ is the
``mother'' graph, $D$ is the ``daughter'' graph and $E$ is an embedding
mechanism. Given a ``host'' graph $H$, a production is applied as follows: 1)
identify one occurrence of $M$ as an isomorphic subgraph of $H$; 2) remove $M$
from $H$ and obtain $H^-$; 3) replace $M$ by an isomorphic copy of $D$; and
finally 4) use the embedding mechanism $E$ to attach $D$ to $H^-$.
\cite{costa16} introduced an efficient graph grammar based on the concept
of {\em distributional semantics}
\citep{harris1954distributional,harris1968mathematical}. Two
key notions are defined: {\em internal core graphs} and {\em interface graphs}. An
internal core graph (or {\em core} for simplicity), denoted $C^v_R(G)$, is a
neighborhood graph of $G$ of radius $R$ rooted in $v$. An interface graph,
denoted $I^v_{R,T}(G)$ is the difference graph of two neighborhood graphs of
$G$ with the same root in $v$ and radius $R+1$ and $R+1+T$. The difference graph
is the graph induced by the set difference of the two vertex sets. Intuitively
an interface corresponds to the ``external shell'' of a core with {\em
thickness} $T$. This shell represents the context available for the definition
of a production rule. The general concepts of mother and daughter graphs are
specialized as follows: given a production $S$, the mother graph $M$ and the
daughter graph $D$ are union graphs of an interface graph and a correspondent
internal core graph with the additional constraint that the interface graphs
in the daughter and mother graph must be identical (up to isomorphism);
finally, the embedding mechanism $E$ is given by the isomorphic bijection
between the interface graphs. In words, new elements are produced by {\em
swapping inner cores} in identical contexts.



% \cite{costa16} introduced a method
% to construct novel graphs according to a distribution, which is given by
% examples. Graphs are vectorized by a \emph{decomposition kernel}
% to train a machine learning model, e.g. an SVM.
% Fragments of the graphs are collected in 
% a \emph{grammar} (analogous to a string grammar) to alter the initial
% set of graphs incrementally. Changes to a graph are evaluated with the
% model. 
% We present a method to increase the flexibility of the graph grammar.

% We give an overview of the graph grammar in \cite{costa16}.
% The grammar consists of sets of interchangeable graph fragments.
% We call these fragments core interface pairs(CIPs) because 
% they consist of a core part and an interface part.
% In terms of a string grammar, core and interface are imaginable 
% as a production: $iiiCCCiii \Longleftrightarrow iiiCCCCiii$.

In Fig. \ref{allcips} (left) a production step is applied to a molecular graph
$G$. The core $C_{R}^v(G)$ (dark) is determined by vertices at maximal
distance $R$ ($R=0$ in the figure) from a chosen root vertex $v$. The
interface $I_{R,T}^v(G)$ with \emph{thickness} $T=2$ is in a lighter shade.
Given a new core-interface pair (CIP) with matching interface, the
substitution can take place to yield the replacement of a carbon with a
nitrogen atom.

\begin{figure}[ht]
      \centering
        \includegraphics[width=0.7\linewidth]{images/allcipsinone.png}
      \caption{
      Core and interface subgraphs are marked in dark and light color
      respectively. Left: Molecular graph with indicated CIP substitution.
      Mid: RNA encoding with nucleotide level resolution.  Right:  RNA encoding with structural element resolution, with symbols: H)airpin, M)ultiloop, S)tem, D)angling end.}
      \label{allcips}
\end{figure}

Note that this type of production rules can be inferred efficiently (i.e. in
linear complexity) from a representative set of graphs.



\subsection{Method extension}

Two problems arise when dealing with complex structures such as RNA polymers:
1) a resolution and 2) a viability issue. Replacing only few nodes at each
iteration can be inefficient when instances consist of several hundreds of
nodes. It is known that for RNAs a more effective description is obtained
considering larger structural components such as {\em stems} (stretches of
consecutive paired nucleotides) and {\em loops} (stretches of consecutive
unpaired nucleotides). As for 2), it is known that the function of RNA
polymers depends on their global structure (i.e. the set of pairs of
interacting nucleotides), which can significantly change when even a single
nucleotide is altered. To deal with these issues we propose two enhancements
to Costa's approach, namely: 1) a grammar coarsening procedure and
2) a constraint integration procedure.


\textbf{Grammar coarsening.} The idea is to allow users to specify in a simple
way a coarsening procedure. We do so via the notion of {\em edge contraction}:
an operation which removes an edge from a graph while simultaneously merging
the two vertices that it previously joined. In addition we allow the more
general notion of {\em vertex contraction}: which removes the restriction that
the contraction must occur over vertices sharing an incident edge. Both
procedures are defined using a node attribute $cid \in \IN$ called {\em
contraction-identifier} and contracting all vertices that share the same $cid$
value. We propose to use the contraction operation as a flexible way to
transform a graph to its coarser version, $G \mapsto G'$. Cores and interfaces
can now be defined exploiting the contracted graph. Starting from a CIP on the
coarse graph, we define the core as the sub graph induced by the vertices of
the original graph that have been contracted to vertices of the core of radius
$R$ in the coarse graph, $C_R^v(G',G)$.  The new interface graph
$I_{R,B}^v(G',G)$ is defined as the Cartesian product of the graph induced by
the nodes adjacent to the core nodes in $G$ at maximal distance $B$ (the
thickness in the base graph) and the interface graph on the coarse graph. In
words, we require that both the interface at base level and at coarse level
match for a core swap to take place. In Fig. \ref{allcips}  we depict a RNA
polymer graph at nucleotide level (center) and its coarse version (right)
where the contraction was informed by the notion of structural component such
as stems, hairpin loops and multiloops. Note that even if the core at coarse
level (in dark) comprises only one node, this corresponds to multiple nodes at
base level. Moreover, while the interface at base level requires only the
presence of few nucleotides, these have to belong to a context defined at the
coarse level that can span a much larger fraction of the instance and can be
viewed as a sort of global positioning constraint.


% In our application case, we contract according to the
% secondary RNA structure and label the resulting vertex accordingly e.g.
% 'Hairpin loop'.$I_{R,T}^v(G')$  encodes far reaching and abstract
% information about the surrounding vertices in our CIP interface matching.
% % graph contration, interface trick
% % change to congruency
% % \textbf{An improvement to the notion of congruency.}
% % CIPs require isomorphism to be considered congruent.
% % We expand this requirement to incorporate
% % the distance to the closest vertices in the core graph, for every interface
% % vertex, when determining if graphs are congruent.
% % %$\forall u \in I_{R,T}^v(G) : 
% % %\underset{z \in  C_{R}^v(G)}{\min} d(u,z) = 
% % %\underset{z' \in  C_{R}^{v'}(G')}{\min} d(\phi(u),z') $
% % \fxwarning{formula style?}
% % $\forall u \in I_{R,T}^v(G) : 
% % \min_{z \in  C_{R}^v(G)} d(u,z) = 
% % \min_{z' \in  C_{R}^{v'}(G')} d(\phi(u),z') $
% % i.e. the distance 
% % to the closest core node is equal for every
% % $u$ and $\phi(u)$.
% % It is easy to see the benefit of this extension.
% % Imagine a CIP embedded into a graph. Let the interface consist of
% % two connected vertices $a$ and $b$, $b$ being adjacent to the core while
% % $a$ is connected to the rest of the graph.
% % Clearly a CIP whose $a$ vertex is connected to its core should not be matched.


% \textbf{Contracting vertices.}
% In fig \ref{allcips} (mid and right, dark) we see how a coarsened 
% graph can be obtained by contraction of edges.
% Dark nodes in the middle picture are contracted to a single 's' labeled
% vertex on the right. 
% Vertices not connected by an edge can also be contracted.
% This is called vertex identification.  
% We did so with vertices in the multi-loop "M" that are not stem "S" vertices 
% (here labeled 'A' and 'G').


\textbf{Constraint integration.}

Specific feasibility constraints can be available for a given domain. For RNAs
the relation between sequence and structure is known to be governed by
thermodynamical forces that seek to minimize the amount of free energy of the
system. The optimal structure can be approximately computed using
sophisticated dynamic programming optimization algorithms. To integrate these
constraints we let the user specify a transformation function which maps instances to feasible instances. In our case the transformation takes a graph representing a candidate RNA structure, removes the pairwise bounds and recomputes them as the solution of a optimal folding algorithm.



% In a simulation step a new graph is generated and an acceptance decision
% has to be made. The new graph is generated by applying a
% production to the current instance. 
% In fig. \ref{allcips}(left) we can see a production being applied
% to the base graph $G$. 
% These changes may alter the coarse graph.
% Therefore the contraction has to be recalculated if a change is accepted.



%\fxwarning{remove this eden part.. what about the directedness? look up what 
%i did!also the hashing part! and then explain it}
%Although the underlying EDeN kernel does not support directed graphs,
%the grammar may contain directed graphs. This is implemented by extracting
%CIPs as if undirected and creating the grammar as usual.
%% need to be able to recalculate abstractions
%A constraint to the contraction is that we need to be able to 
%generate it algorithmically. In the sampling phase we recalculate 
%the contraction after changes to the underlying graph.


\section{Empirical evaluation}

%%%%%%%
%% RNA AND FOLDING 
%%%%%%%
Ribonucleic acid (RNA) sequences are sequences over the four nucleotides guanine,
uracil, adenine, and cytosine (G,U,A,C).
The nucleotides are connected to a ribose-phosphate backbone which imposes a
direction. Non coding RNA (ncRNA) is transcribed from DNA but not
translated into proteins. Instead it serves various other functions.
The function depends on the sequence as well as the structure in which it 
\emph{folds} by connecting non adjacent nucleotides via hydrogen bonds. 
RNAs are grouped into functional families\cite{rfam}.
Calculating the physically optimal folding for a single sequence
is possible, but actual sequences will often fold in unison with 
sequences in their family. Therefore, if we are to fold a sequence,
we find the four most similar sequences from the training set, align them  and 
look for a consensus folding.
This is done by nearest neighbor searching the four closest sequences, 
aligning the sequences with \emph{muscle} \cite{muscle}
and then folding this alignment with \emph{RNAalifold}
\cite{rnaalifold}.  
The result is a \emph{secondary structure} which is translated into a 
graph. First a directed path graph is generated which is labeled
according to the nucleotides in the sequence, then undirected edges 
are introduced that represent the hydrogen bonds.
The presented grammar is compatible with directed graphs.

%%%%%%%%
% INFERNAL STUFF AND PARAMOPT
%%%%%%%%
\emph{Infernal}\cite{infernal} can classify members of these families using
covariance models. Infernal gives us a domain specific way to evaluate
the generated graphs. 
% infernal stuff
For each family Infernal provides a model which is trained on 
a hand curated list of family representatives (seeds) as well as a 
bit score threshold. Scoring a sequence with the model yields a bit score.
Is this score higher than the threshold, the sequence should be considered 
part of the family. 
% choose params
We chose our train and test RNA sequences among the seed sequences of the 
families. 
Sequences were chosen from RNA families with three properties:
\fxwarning{how to enumerate?}
1) many members; because some families contain 10
instances which is very few for a classification task. 2)  interesting 
structure; many sequences fold into structures that exhibit a large number of 
unpaired bases which would result in a very simple contracted graph
3) similar length; because classification should no be trivial.
In the sampling process, many variables are involved.
%paramoptimisation 
To obtain 
\fxwarning{this might become obsolete since optimisation was only for the first dataset..
gl is more unstable on other sets -> params should be more conservative...
-> i hand picked a conservative dataset..
also note to self: dont merge the curve from 'clevo.. also: i set the testsizes from 50 to 100 there..
}
sampling parameters, the parameter space was randomly searched.
To obtain the grammar parameters, a grid search over some options was conducted.




%%%%%%%%%%%%%%%%%
% About the pics
%%%%%%%%%%%%%%%%%
%infevel 
Fig. \ref{infeval} shows the average bit-score of generated graphs
and the size of the training set. The black bar represents the family specific 
threshold. In blue we see the scores of the sequences generated with the
enhanced grammar. In red the sequences generated by a normal grammar. 
\fxwarning{how to enumerate? should i mention that the light color is the std?}
Two results are noticeable: 1) the sequences generated by our method
perform overwhelmingly above the threshold. 2) the new method outperforms
the old. 
% eval learning curve+ edit dist
\fxwarning{is this the right citation?}
Costa \cite{costa16} defines \emph{the constructive learning problem for 
finite samples}.
The goal is to generate graphs that are different from the training set,
but depict a similar distribution.
The similarity of two sets of sequences $a$ and $b$ 
is obtained with $s(a,b)/\sqrt{(s(a,a)*s(b,b))}$ where $s$ is the mean of the
similarity matrix between the two arguments. If the arguments are identical,
the diagonal is removed from the matrix.
Distributions are indirectly comparable by looking at the performance 
of trained estimators. A high similarity in performance indicates a high
similarity in distribution.
Fig \ref{learncurve} shows the average (over positive and negative generated sets)
similarity as described above. 
We also see the learning curves obtained by training an estimator
on $x$ random instances from each of two RNA family representatives.
We see the curves for the original seeds, the sequences generated by 
our graph generator and the combined set. 
\fxwarning{drop a sentence on the interpretation}






\begin{figure}[ht]
      \centering
  \begin{minipage}[b]{0.47\textwidth}
    \includegraphics[width=\textwidth]{images/infernal_abstr.png}
    \caption{Infernal scores of generated sequences for RNAs of family RF01725.}
      \label{infeval}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.52\textwidth}
    \includegraphics[width=\textwidth]{images/learningcurve.png}
    \caption{Learning curve for classification between RF00005 and RF00162.vs}
     \label{learncurve}
  \end{minipage}
\end{figure}

\fxwarning{pictures are not in their final form}

%\begin{figure}[ht]
%      \centering
%        \includegraphics[width=0.5\linewidth]{images/learningcurve.png}
%      \caption{learning curve}
%      \label{learncurve}
%\end{figure}


 
\section{Discussion and Conclusion}

We have introduced a flexible approach to tackle the problem of long range
dependency modeling for a constructive machine learning approach. Allowing for
a user defined coarsening procedure we have showed how we can synthesize novel
RNA sequences that are functionally equivalent to an original example sample.
The coarsening procedure allows the injection of domain knowledge,
significantly improving the quality of the results over the original domain a-specific approach. In future work we will investigate how to learn task
specific coarsening schemes in a supervised fashion directly from data.


\bibliography{mybib}


\end{document}

